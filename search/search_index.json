{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Machine Learning","text":""},{"location":"#machine-learning","title":"Machine Learning","text":"Versions 2025.2"},{"location":"classes/concepts/data/data/","title":"1.3. Data","text":"<p>All the concepts of Machine Learning are based on data. The quality and quantity of available data are fundamental to the success of any machine learning model. In this context, it is important to understand how data is structured, processed, and used to train models.</p>"},{"location":"classes/concepts/data/data/#nature-of-data","title":"Nature of Data","text":"<p>Data can be thought of as a collection of features or attributes that describe a particular phenomenon or object. In machine learning, data is often represented as a matrix, where each row corresponds to an example and each column corresponds to a feature. This representation is known as the feature space.</p> <p>A feature is a measurable property or characteristic of the phenomenon being studied. Features can be numerical (e.g., height, weight) or categorical (e.g., color, type). The set of features used to describe the data is known as the feature set or feature vector. Features are used to describe the data and can be used as input to machine learning models. Each feature is a dimension of the feature space, and the dataset is represented as a point in this space.</p> <p>Features can be numerical or categorical:</p> <ul> <li>Numerical features are those that can take continuous values, such as height or weight;</li> <li>Categorical features are those that take discrete values, such as color or type.</li> </ul> <p>Depending on the type of machine learning algorithm, features may be treated differently. For example, some algorithms work better with numerical features, while others are more suited for categorical features. In this context, it is necessary to convert categorical features into a format that algorithms can understand, such as using one-hot encoding<sup>1</sup> or label encoding<sup>2</sup>.</p> <p>Additionally, numerical features, such as height or weight, are often normalized to ensure that all features contribute equally to the model. Normalization is a preprocessing technique that adjusts the values of features to a common scale, typically between 0 and 1 or -1 and 1. A common approach to normalization is min-max scaling, which transforms the data by subtracting the minimum value and dividing by the range (maximum - minimum). This ensures that all features are on the same scale and can improve the performance of many machine learning algorithms.</p>"},{"location":"classes/concepts/data/data/#datasets","title":"Datasets","text":"<p>Data is often stored in datasets, which are structured collections of data that can be easily accessed, managed, and updated. Datasets can be relational (e.g., SQL databases) or non-relational (e.g., NoSQL databases). Relational datasets store data in tables with predefined schemas, while non-relational datasets allow for more flexible data structures. Some common types of datasets used in machine learning include:</p> <ul> <li>UCI Machine Learning Repository: a collection of datasets for machine learning tasks, including classification, regression, and clustering.</li> <li>Kaggle Datasets: a platform that offers a wide variety of datasets for different machine learning tasks, from text classification to image recognition.</li> <li>OpenML: a collaborative platform for sharing and organizing machine learning datasets and experiments.</li> <li>Google Dataset Search: a search engine for datasets across the web, allowing users to find datasets for various machine learning tasks.</li> <li>AWS Open Data Registry: a collection of publicly available datasets hosted on Amazon Web Services, covering a wide range of domains, including climate, healthcare, and transportation.</li> <li>Data.gov: a repository of datasets provided by the U.S. government, covering various topics such as agriculture, health, and energy.</li> <li>FiveThirtyEight Data: a collection of datasets used in articles by FiveThirtyEight, covering topics such as politics, sports, and economics.</li> <li>Awesome Public Datasets: a curated list of high-quality public datasets for various domains.</li> <li>The World Bank Open Data: a collection of global development data, including economic, social, and environmental indicators.</li> <li>IMDB Datasets: a collection of datasets related to movies, TV shows, and actors, useful for natural language processing and recommendation systems.</li> <li>Yelp Open Dataset: a dataset containing business reviews, user data, and check-ins, useful for sentiment analysis and recommendation systems.</li> </ul>"},{"location":"classes/concepts/data/data/#data-quality","title":"Data Quality","text":"<p>Data quality is a critical aspect of machine learning, as the performance of models heavily depends on the quality of the data used for training. Poor quality data can lead to inaccurate predictions and unreliable models. Common issues with data quality include:</p> <ul> <li>Missing data: values that are not available for some variables;</li> <li>Duplicate data: records that appear more than once in the dataset;</li> <li>Noisy data: values that are inconsistent or incorrect;</li> <li>Imbalanced data: when one class is much more frequent than another, which can lead to a biased model.</li> <li>Inconsistent data: when the data does not follow a consistent pattern or format, making it difficult to analyze and train the model.</li> <li>Irrelevant data: variables that do not contribute to the machine learning task and may harm the model's performance.</li> </ul> <p>To address these issues, it is common to perform a data cleaning and preprocessing process, which may include:</p> <ul> <li>Removing missing data: excluding records with missing values or imputing values based on other observations.</li> <li>Removing duplicates: identifying and removing duplicate records.</li> <li>Handling noisy data: applying smoothing or filtering techniques to reduce noise in the data.</li> <li>Balancing classes: techniques such as undersampling or oversampling - data augmentation<sup>6</sup> - to deal with imbalanced classes.</li> <li>Normalization: adjusting the values of variables to a common scale, ensuring that all variables contribute equally to the model.</li> <li>Transforming variables: applying techniques such as logarithm, square root, or Box-Cox to transform non-linear variables into linear ones.</li> <li>Encoding categorical variables: converting categorical variables into a format that algorithms can understand, such as using one-hot encoding or label encoding.</li> </ul> <p>Additionally, it is important to consider the order of the data, especially in time series problems, where the sequence of the data is crucial for analysis and modeling.</p>"},{"location":"classes/concepts/data/data/#data-volume-and-balance","title":"Data Volume and Balance","text":"<p>The volume and balance of data are also important factors to consider in machine learning. Data volume refers to the amount of data available for training and testing machine learning models. The larger the volume of data, the more information the model can learn, which usually results in better performance. However, it is also important to consider the quality of the data, as noisy or irrelevant data can harm the model's performance.</p> <p>Additionally, it is important to consider class balancing, especially in classification problems. Class balancing refers to the equitable distribution of classes in the dataset. If one class is much more frequent than another, this can lead to a biased model, which tends to predict the majority class. To address this issue, techniques such as undersampling or oversampling can be used to balance the classes. Undersampling involves removing records from the majority class, while oversampling involves duplicating records from the minority class or generating synthetic data.</p> <p>For supervised learning models, it is essential to have a labeled dataset, where each example has an input (features) and an output (label). This allows the model to learn to map the inputs to the correct outputs.</p> <p>Furthermore, the data can be classified into three main categories:</p> Set Description Train Used to train the model, allowing it to learn the patterns and relationships between features and labels. Test Used to tune the model's hyperparameters and prevent overfitting, ensuring it generalizes well to new examples. Validation Used to evaluate the model's performance on unseen data, ensuring it generalizes well to new examples."},{"location":"classes/concepts/data/data/#some-examples-of-datasets","title":"Some Examples of Datasets","text":""},{"location":"classes/concepts/data/data/#salmon-vs-seabass","title":"Salmon vs Seabass","text":"<p>A fictional dataset about salmon and seabass, where each record is labeled as \"salmon\" or \"seabass\". The goal is to better understand how the data can be used to differentiate between the two species. In this context, the features may include, for example: size and brightness<sup>5</sup>.</p>"},{"location":"classes/concepts/data/data/#problem","title":"Problem","text":"<p>Imagine you have a fish sorting machine. Every day, fishing boats dump tons of fish onto a conveyor belt, and the goal of the machine is to separate the fish, classifying them as \"salmon\" or \"seabass\" based on their characteristics.</p> <p>The conveyor belt has sensors that measure the size and brightness of the fish. Based on these measurements, the machine must decide whether the fish is a salmon or a seabass.</p> \\[ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix} \\] <p>where \\(x_1\\) is the size of the fish and \\(x_2\\) is the brightness of the fish. The machine must learn to classify the fish based on these characteristics, using a function \\(f\\) that maps the input features to the output class: salmon or seabass.</p>"},{"location":"classes/concepts/data/data/#sample-data","title":"Sample Data","text":"<p>To better understand the data, a sample of fish was taken, where each fish is described by its size and brightness characteristics. The table below presents a sample of the collected data:</p> Size (cm) Brightness (0-10) Species 60 6 salmon 45 5 seabass 78 7 salmon 90 5.2 salmon 71 9 salmon 80 3 seabass 64 6 salmon 58 2 seabass 63 6.8 seabass 50 4 seabass <p>When plotting the data, we can visualize the size and brightness of each fish in a two-dimensional space. Each fish is represented by a point in this space, where the x-axis represents the size and the y-axis represents the brightness. The points are colored according to their species: salmon or seabass.</p> 2025-09-03T22:44:43.873603 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p>Sample data of salmon and seabass, where each fish is described by its size and brightness characteristics. The points are colored according to their species: salmon (blue) or seabass (orange). For 1-dimensional data, the points are plotted along the x-axis, representing the size and brightness of the fish.</p> 2025-09-03T22:44:43.932398 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p>Sample data of salmon and seabass, where each fish is described by its size and brightness characteristics. The points are colored according to their species: salmon (blue) or seabass (orange). The x-axis represents the size of the fish, while the y-axis represents its brightness.</p> <p>The machine must learn to draw a line that separates the two classes, salmon and seabass, based on the size and brightness characteristics. This line is called a decision boundary. So that, as soon as a new fish is placed on the conveyor belt, the machine can decide whether it is a salmon or a seabass based on its size and brightness characteristics - as shown in the figure on the right.</p> <p>In general, in the context of classification, the machine must learn to draw decision boundaries in a multidimensional feature space. Allowing, when a new example is presented, the machine to decide which class it belongs to based on the characteristics of the example.</p> <p>Attention</p> <p>The decision boundary is not always linear. In some cases, the data may be distributed in a way that requires a non-linear decision boundary to separate the classes effectively. In such cases, more complex models, such as neural networks or support vector machines with kernels, may be needed to find an appropriate separation.</p>"},{"location":"classes/concepts/data/data/#iris-dataset","title":"Iris Dataset","text":"<p>UCI Machine Learning Repository: the Iris Dataset is a classic dataset used for classification tasks in machine learning. It was introduced by Sir Ronald A. Fisher in 1936<sup>3</sup> and has since become one of the most widely used datasets in the field<sup>4</sup>.</p> <p>The Iris Dataset is a classic and real dataset used for flower classification. It contains 150 samples of three different species of Iris flowers (Iris setosa, Iris versicolor, and Iris virginica), with four features: petal and sepal length and width.</p> <p></p> <p>The dataset is widely used to demonstrate machine learning algorithms, especially for classification tasks. It is simple enough to be easily understood, but also presents interesting challenges for more complex models.</p> <p>A sample of the Iris dataset is presented in the table below:</p> sepal length(cm) sepal width(cm) petal length(cm) petal width(cm) class 5.7 3.0 4.2 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.2 2.9 4.3 1.3 versicolor 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 6.7 3.0 5.2 2.3 virginica 6.3 2.5 5.0 1.9 virginica 6.5 3.0 5.2 2.0 virginica <p>Sample of the Iris dataset, containing features such as sepal length, sepal width, petal length, and petal width, along with the class of the flower. The dataset is widely used for classification tasks in machine learning.</p> <p>Below there is a code snippet that loads the Iris dataset using the <code>pandas</code> library and visualizes it using <code>matplotlib</code>. The dataset is loaded from a CSV file, and the features are plotted in a scatter plot, with different colors representing the different classes of flowers.</p> <p> </p> Editor (session: default) Run <pre>import pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Carregar o conjunto de dados Iris\niris = load_iris()\n\n# Transforma em DataFrame\ndf = pd.DataFrame(\n    data=iris.data,\n    columns=['sepal_l', 'sepal_w', 'petal_l', 'petal_w']\n)\ndf['class'] = iris.target_names[iris.target]\n\n# Imprime os dados\nprint(df)</pre> Output Clear <pre><code></code></pre> <p></p> <p>Also, the dataset can be visualized using the <code>seaborn</code> library, which provides a high-level interface for drawing attractive statistical graphics:</p> 2025-09-03T22:44:46.542533 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p>Dataset visualization of the Iris dataset using the <code>seaborn</code> library. The scatter plot shows the relationship between the features of the flowers, with different colors representing the different classes. The diagonal plots show the distribution of each feature, allowing for a better understanding of the data.</p> <p>In this visualization, each feature is represented by an axis, and the flowers are plotted in a multidimensional space. The colors represent the different classes of flowers, allowing for the identification of patterns and separations between the classes. Note that for some configurations, such as petal length vs petal width, the classes are well separated, while in others, such as sepal length vs sepal width, the classes overlap.</p> <p>Real World</p> <p>The Iris dataset is a classic example of a dataset used in machine learning, particularly for classification tasks. It is simple enough to be easily understood, but also presents interesting challenges for more complex models. The dataset is widely used in educational contexts to teach concepts of machine learning and data analysis.</p> <p>One can imagine that in more complex problems, such as image recognition or natural language processing, the data can be much more complex and challenging. Not allowing for a clear visualization of the spatial distribution of features. However, the fundamental principles of machine learning remain the same: understanding the data, properly preprocessing it, and choosing the right model for the task.</p>"},{"location":"classes/concepts/data/data/#other-datasets","title":"Other Datasets","text":"<p>Data distribution is a crucial aspect of machine learning, as it directly affects the model's ability to learn and generalize. Usually, the nature of the data can be visualized in scatter plots, histograms, or boxplots, allowing for the identification of patterns, trends, and anomalies in the data - of course, when the data has a low number of dimensions (2 or 3).</p> <p>Illustrations of some distributions with only two dimensions are presented below:</p> 2025-09-03T22:44:47.013906 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p>Data distributions in two dimensions in different spatial formats. For each surface, the separation between classes is made based on the characteristics of the data. The distribution of the data can affect the model's ability to learn and generalize.</p> <p>The figure above presents four different data distributions in two dimensions, each with its own spatial characteristics. The separation between classes is made based on the characteristics of the data, and the distribution of the data can affect the model's ability to learn and generalize. In general, the function of a machine learning technique is to find a separation between classes in order to maximize the model's accuracy.</p>"},{"location":"classes/concepts/data/data/#summary","title":"Summary","text":"<p>Data is the foundation of any machine learning model. The quality, quantity, and nature of the available data are critical to the model's success. It is important to understand how the data is structured, processed, and used to train models, as well as to consider the volume of data and the balance of classes.</p> <p>In addition, it is essential to perform proper data preprocessing, which may include cleaning, transformation, and normalization, to ensure that models can learn effectively and make accurate predictions.</p> <p>The great challenge in machine learning is to seek the best separation between classes in order to maximize the model's accuracy. This involves not only the choice of algorithm but also a deep understanding of the data and the relationships between variables.</p> <ol> <li> <p>One-Hot Encoding - Wikipedia \u21a9</p> </li> <li> <p>Label Encoding - Scikit-learn \u21a9</p> </li> <li> <p>Fisher, R. A.. 1936. Iris. UCI Machine Learning Repository. https://doi.org/10.24432/C56C76. \u21a9</p> </li> <li> <p>Iris Dataset - Wikipedia \u21a9</p> </li> <li> <p>Richard O. Duda, Peter E. Hart, and David G. Stork. 2000. Pattern Classification (2nd Edition). Wiley-Interscience, USA.\u00a0\u21a9</p> </li> <li> <p>Data Augmentation - Wikipedia \u21a9</p> </li> </ol>"},{"location":"classes/concepts/data/distributions/","title":"Distributions","text":"In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>from io import StringIO\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef twospirals(n_points, noise=0.7):\n    n = np.sqrt(np.random.rand(n_points, 1)) * 780 * (2 * np.pi) / 360\n    d1x = -np.cos(n) * n + np.random.rand(n_points, 1) * noise\n    d1y = np.sin(n) * n + np.random.rand(n_points, 1) * noise\n    return (np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y)))), \n            np.hstack((np.zeros(n_points), np.ones(n_points))))\n\n# definindo o tamanho da figura\nfig, ax = plt.subplots(2, 2, figsize=(12, 12))\n\nfor l in range(2):\n    for c in range(2):\n        ax[l][c].xaxis.set_visible(False)\n        ax[l][c].yaxis.set_visible(False)\n        ax[l][c].set_aspect('equal', adjustable='box')\n        ax[l][c].set_xlim(-7, 7)\n        ax[l][c].set_ylim(-7, 7)\n\nN = 1000\n</pre> from io import StringIO  import numpy as np import matplotlib.pyplot as plt  def twospirals(n_points, noise=0.7):     n = np.sqrt(np.random.rand(n_points, 1)) * 780 * (2 * np.pi) / 360     d1x = -np.cos(n) * n + np.random.rand(n_points, 1) * noise     d1y = np.sin(n) * n + np.random.rand(n_points, 1) * noise     return (np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y)))),              np.hstack((np.zeros(n_points), np.ones(n_points))))  # definindo o tamanho da figura fig, ax = plt.subplots(2, 2, figsize=(12, 12))  for l in range(2):     for c in range(2):         ax[l][c].xaxis.set_visible(False)         ax[l][c].yaxis.set_visible(False)         ax[l][c].set_aspect('equal', adjustable='box')         ax[l][c].set_xlim(-7, 7)         ax[l][c].set_ylim(-7, 7)  N = 1000 In\u00a0[\u00a0]: Copied! <pre>x1, y1 = np.random.multivariate_normal(\n    [-2.5, -2.5],\n     [[1, 0], [0, 1]],\n    N\n).T\n\nx2, y2 = np.random.multivariate_normal(\n    [2.5, 2.5],\n    [[1, 0], [0, 1]],\n    N\n).T\n\nax[0][0].plot(\n    x1, y1, '.',\n    x2, y2, '.'\n)\n</pre> x1, y1 = np.random.multivariate_normal(     [-2.5, -2.5],      [[1, 0], [0, 1]],     N ).T  x2, y2 = np.random.multivariate_normal(     [2.5, 2.5],     [[1, 0], [0, 1]],     N ).T  ax[0][0].plot(     x1, y1, '.',     x2, y2, '.' ) In\u00a0[\u00a0]: Copied! <pre>x3, y3 = np.random.multivariate_normal(\n    [-2.5, 2.5],\n     [[1, 0], [0, 1]],\n    N\n).T\n\nx4, y4 = np.random.multivariate_normal(\n    [2.5, -2.5],\n    [[1, 0], [0, 1]],\n    N\n).T\n\nax[0][1].plot(\n    np.hstack((x1, x2)), np.hstack((y1, y2)), '.',\n    np.hstack((x3, x4)), np.hstack((y3, y4)), '.'\n)\n</pre> x3, y3 = np.random.multivariate_normal(     [-2.5, 2.5],      [[1, 0], [0, 1]],     N ).T  x4, y4 = np.random.multivariate_normal(     [2.5, -2.5],     [[1, 0], [0, 1]],     N ).T  ax[0][1].plot(     np.hstack((x1, x2)), np.hstack((y1, y2)), '.',     np.hstack((x3, x4)), np.hstack((y3, y4)), '.' ) In\u00a0[\u00a0]: Copied! <pre>xc, yc = np.random.multivariate_normal(\n    [0, 0],\n    [[1, 0], [0, 1]],\n    N\n).T\n\nnoise = .5\nradius = 2.5\ntheta = np.linspace(0, 2 * np.pi, N)\n\nxt = radius * np.cos(theta)\nyt = radius * np.sin(theta)\n\nxt = xt + np.random.normal(xt, noise)\nyt = yt + np.random.normal(yt, noise)\n\n# Plot the surface\nax[1][0].plot(\n    xc, yc, '.',\n    xt, yt, '.'\n)\n</pre> xc, yc = np.random.multivariate_normal(     [0, 0],     [[1, 0], [0, 1]],     N ).T  noise = .5 radius = 2.5 theta = np.linspace(0, 2 * np.pi, N)  xt = radius * np.cos(theta) yt = radius * np.sin(theta)  xt = xt + np.random.normal(xt, noise) yt = yt + np.random.normal(yt, noise)  # Plot the surface ax[1][0].plot(     xc, yc, '.',     xt, yt, '.' ) In\u00a0[\u00a0]: Copied! <pre>X, y = twospirals(N)\nax[1][1].plot(X[y == 0, 0], X[y == 0, 1], '.')\nax[1][1].plot(X[y == 1, 0], X[y == 1, 1], '.')\n</pre> X, y = twospirals(N) ax[1][1].plot(X[y == 0, 0], X[y == 0, 1], '.') ax[1][1].plot(X[y == 1, 0], X[y == 1, 1], '.') In\u00a0[\u00a0]: Copied! <pre>plt.axis('equal')\nplt.subplots_adjust(wspace=0, hspace=0)\n\n# Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</pre> plt.axis('equal') plt.subplots_adjust(wspace=0, hspace=0)  # Display the plot buffer = StringIO() plt.savefig(buffer, format=\"svg\", transparent=True) print(buffer.getvalue())"},{"location":"classes/concepts/data/iris_data/","title":"Iris data","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom sklearn.datasets import load_iris\n</pre> import pandas as pd from sklearn.datasets import load_iris In\u00a0[\u00a0]: Copied! <pre># Carregar o conjunto de dados Iris\niris = load_iris()\n</pre> # Carregar o conjunto de dados Iris iris = load_iris() In\u00a0[\u00a0]: Copied! <pre># Transforma em DataFrame\ndf = pd.DataFrame(\n    data=iris.data,\n    columns=['sepal_l', 'sepal_w', 'petal_l', 'petal_w']\n)\ndf['class'] = iris.target_names[iris.target]\n</pre> # Transforma em DataFrame df = pd.DataFrame(     data=iris.data,     columns=['sepal_l', 'sepal_w', 'petal_l', 'petal_w'] ) df['class'] = iris.target_names[iris.target] In\u00a0[\u00a0]: Copied! <pre># Imprime os dados\nprint(df)\n</pre> # Imprime os dados print(df)"},{"location":"classes/concepts/data/iris_visualization/","title":"Iris visualization","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> import pandas as pd import matplotlib.pyplot as plt import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>from io import StringIO\nfrom sklearn.datasets import load_iris\n</pre> from io import StringIO from sklearn.datasets import load_iris In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(12, 10))\n</pre> plt.figure(figsize=(12, 10)) In\u00a0[\u00a0]: Copied! <pre># Carregar o conjunto de dados Iris\niris = load_iris()\n</pre> # Carregar o conjunto de dados Iris iris = load_iris() In\u00a0[\u00a0]: Copied! <pre># Transforma em DataFrame\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ndf['target'] = iris.target_names[iris.target]\n</pre> # Transforma em DataFrame df = pd.DataFrame(data=iris.data, columns=iris.feature_names) df['target'] = iris.target_names[iris.target] In\u00a0[\u00a0]: Copied! <pre># Visualizar o conjunto de dados Iris\nsns.pairplot(df, hue='target', height=3)\n</pre> # Visualizar o conjunto de dados Iris sns.pairplot(df, hue='target', height=3) In\u00a0[\u00a0]: Copied! <pre># Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</pre> # Para imprimir na p\u00e1gina HTML buffer = StringIO() plt.savefig(buffer, format=\"svg\", transparent=True) print(buffer.getvalue())"},{"location":"classes/concepts/data/salmon_vs_seabass_1/","title":"Salmon vs seabass 1","text":"In\u00a0[\u00a0]: Copied! <pre>from io import StringIO\n</pre> from io import StringIO In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre># features of salmons (length and lightness)\nsalmon = [\n    [60, 6], [78, 7], [90, 5.2], [71, 9], [64, 6]\n]\nseabass = [\n    [45, 5], [80, 3], [58, 2], [63, 6.8], [50, 4]\n]\n</pre> # features of salmons (length and lightness) salmon = [     [60, 6], [78, 7], [90, 5.2], [71, 9], [64, 6] ] seabass = [     [45, 5], [80, 3], [58, 2], [63, 6.8], [50, 4] ] In\u00a0[\u00a0]: Copied! <pre>zeros = [0] * len(salmon)\n</pre> zeros = [0] * len(salmon) In\u00a0[\u00a0]: Copied! <pre># definindo o tamanho da figura\nfig, ax = plt.subplots(1, 2, figsize=(7, 1))\n</pre> # definindo o tamanho da figura fig, ax = plt.subplots(1, 2, figsize=(7, 1)) In\u00a0[\u00a0]: Copied! <pre>for i in range(len(ax)):\n    ax[i].set_frame_on(False)\n    ax[i].yaxis.set_visible(False)\n    ax[i].spines['left'].set_position('zero')\n    ax[i].spines['bottom'].set_position('zero')\n    ax[i].set_xlabel(['Length', 'Brightness'][i])\n\n    ax[i].plot(\n        [[40, 100], [0, 10]][i], [0, 0], 'k',\n        [fish[i] for fish in salmon], zeros, 'o',\n        [fish[i] for fish in seabass], zeros, 'o'\n    )\n</pre> for i in range(len(ax)):     ax[i].set_frame_on(False)     ax[i].yaxis.set_visible(False)     ax[i].spines['left'].set_position('zero')     ax[i].spines['bottom'].set_position('zero')     ax[i].set_xlabel(['Length', 'Brightness'][i])      ax[i].plot(         [[40, 100], [0, 10]][i], [0, 0], 'k',         [fish[i] for fish in salmon], zeros, 'o',         [fish[i] for fish in seabass], zeros, 'o'     ) In\u00a0[\u00a0]: Copied! <pre># Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</pre> # Display the plot buffer = StringIO() plt.savefig(buffer, format=\"svg\", transparent=True) print(buffer.getvalue())"},{"location":"classes/concepts/data/salmon_vs_seabass_2/","title":"Salmon vs seabass 2","text":"In\u00a0[\u00a0]: Copied! <pre>from io import StringIO\n</pre> from io import StringIO In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>salmon = [\n    [60, 6], [78, 7], [90, 5.2], [71, 9], [64, 6]\n]\nseabass = [\n    [45, 5], [80, 3], [58, 2], [63, 6.8], [50, 4]\n]\n</pre> salmon = [     [60, 6], [78, 7], [90, 5.2], [71, 9], [64, 6] ] seabass = [     [45, 5], [80, 3], [58, 2], [63, 6.8], [50, 4] ] In\u00a0[\u00a0]: Copied! <pre>zeros = [0] * len(salmon)\n</pre> zeros = [0] * len(salmon) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n</pre> fig, ax = plt.subplots(1, 2, figsize=(8, 4)) In\u00a0[\u00a0]: Copied! <pre>for i in range(len(ax)):\n    ax[i].plot(\n\n        [fish[0] for fish in salmon],\n        [fish[1] for fish in salmon],\n        'o',\n\n        [fish[0] for fish in seabass],\n        [fish[1] for fish in seabass],\n        'o',\n\n        [40, 100], [7, 3], '--m', lw=3\n\n    )\n    ax[i].set_xlim(40, 100)\n    ax[i].set_ylim(0, 10)\n    ax[i].set_xlabel('Length')\n    ax[i].set_ylabel('Brightness')\n</pre> for i in range(len(ax)):     ax[i].plot(          [fish[0] for fish in salmon],         [fish[1] for fish in salmon],         'o',          [fish[0] for fish in seabass],         [fish[1] for fish in seabass],         'o',          [40, 100], [7, 3], '--m', lw=3      )     ax[i].set_xlim(40, 100)     ax[i].set_ylim(0, 10)     ax[i].set_xlabel('Length')     ax[i].set_ylabel('Brightness') In\u00a0[\u00a0]: Copied! <pre>ax[1].plot(70, 4, 'xg', markersize=12)\n</pre> ax[1].plot(70, 4, 'xg', markersize=12) In\u00a0[\u00a0]: Copied! <pre># Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</pre> # Display the plot buffer = StringIO() plt.savefig(buffer, format=\"svg\", transparent=True) print(buffer.getvalue())"},{"location":"classes/concepts/exercise/exercise/","title":"1.4. Exercise","text":""},{"location":"classes/concepts/exercise/exercise/#to-do","title":"To Do","text":"<p>Quiz</p> <p>Now, you can practice what you learned in this lesson.</p> <p>Answer the questions in the LMS about the concepts presented here.</p>"},{"location":"classes/concepts/kdd/kdd/","title":"1.2. KDD","text":"<p>Knowledge Discovery in Databases (KDD) \u00e9 o processo de identificar padr\u00f5es v\u00e1lidos, \u00fateis e compreens\u00edveis em grandes conjuntos de dados, combinando t\u00e9cnicas de minera\u00e7\u00e3o de dados, estat\u00edstica, aprendizado de m\u00e1quina e gerenciamento de banco de dados. \u00c9 um processo iterativo e interativo, amplamente utilizado em \u00e1reas como ci\u00eancia de dados, intelig\u00eancia de neg\u00f3cios e pesquisa cient\u00edfica. O KDD foi formalizado por Fayyad et al. (1996) e \u00e9 composto por v\u00e1rias etapas, que podem variar ligeiramente dependendo do contexto, mas geralmente seguem a estrutura abaixo. A seguir, as etapas principais:</p>"},{"location":"classes/concepts/kdd/kdd/#etapas-do-processo-kdd","title":"Etapas do Processo KDD","text":"<pre><code>flowchart TD\n    A@{ shape: database, label: \"1 - Data Selection\"} --&gt; B@{ shape: procs, label: \"2 - Data Preprocessing\"}\n    B --&gt; C@{ shape: notch-pent, label: \"3 - Data Transformation\"}\n    C --&gt; D@{ shape: subproc, label: \"4 - Data Mining\"}\n    D --&gt; E@{ shape: notch-rect, label: \"5 - Evaluation and Interpretation\"}\n    E --&gt; F@{ shape: doc, label: \"6 - Presentation and Use of Knowledge\"}\n    F --&gt; A\n    F --&gt; D</code></pre>"},{"location":"classes/concepts/kdd/kdd/#1-data-selection","title":"1. Data Selection","text":"Descri\u00e7\u00e3o Detalhes Objetivo Identificar e extrair um subconjunto relevante de dados do reposit\u00f3rio (banco de dados, data warehouse, etc.) para an\u00e1lise. Atividades - Compreender o problema de neg\u00f3cio ou a quest\u00e3o de pesquisa.- Definir os atributos (vari\u00e1veis) e inst\u00e2ncias (registros) relevantes.- Filtrar dados com base em crit\u00e9rios espec\u00edficos, como per\u00edodo de tempo, localiza\u00e7\u00e3o ou tipo de cliente. Exemplo Em um banco de dados de vendas, selecionar apenas transa\u00e7\u00f5es de clientes de uma regi\u00e3o espec\u00edfica em um determinado ano. Ferramentas Consultas SQL, ferramentas de ETL (Extract, Transform, Load). Desafios Garantir que os dados selecionados sejam representativos e suficientes para o problema."},{"location":"classes/concepts/kdd/kdd/#2-data-preprocessing","title":"2. Data Preprocessing","text":"Descri\u00e7\u00e3o Detalhes Objetivo Preparar os dados brutos para an\u00e1lise, tratando inconsist\u00eancias, ru\u00eddos e dados ausentes. Atividades - Limpeza de dados: Corrigir erros (e.g., valores inconsistentes, duplicatas) e tratar valores ausentes (imputa\u00e7\u00e3o, exclus\u00e3o ou interpola\u00e7\u00e3o).- Integra\u00e7\u00e3o de dados: Combinar dados de fontes heterog\u00eaneas, resolvendo conflitos de formato ou schema.- Redu\u00e7\u00e3o de dados: Eliminar redund\u00e2ncias ou reduzir dimensionalidade (e.g., usando PCA ou sele\u00e7\u00e3o de atributos). Exemplo Substituir valores nulos em uma coluna de idade por uma m\u00e9dia ou mediana, ou normalizar pre\u00e7os para uma mesma moeda. Ferramentas Pandas (Python), R, scripts de limpeza personalizados. Desafios Preservar a integridade dos dados e evitar introdu\u00e7\u00e3o de vi\u00e9s durante a limpeza."},{"location":"classes/concepts/kdd/kdd/#3-data-transformation","title":"3. Data Transformation","text":"Descri\u00e7\u00e3o Detalhes Objetivo Converter os dados em um formato adequado para as t\u00e9cnicas de minera\u00e7\u00e3o. Atividades - Normaliza\u00e7\u00e3o (e.g., escalonamento para [0,1]) ou padroniza\u00e7\u00e3o (m\u00e9dia 0, desvio padr\u00e3o 1).- Discretiza\u00e7\u00e3o de vari\u00e1veis cont\u00ednuas (e.g., transformar idades em faixas et\u00e1rias).- Cria\u00e7\u00e3o de novas vari\u00e1veis (feature engineering), como calcular a raz\u00e3o entre duas vari\u00e1veis.- Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas (e.g., one  -hot encoding). Exemplo Transformar uma coluna de datas em vari\u00e1veis como \"dia da semana\" ou \"m\u00eas\". Ferramentas Scikit-learn, SQL, ferramentas de ETL. Desafios Escolher transforma\u00e7\u00f5es que maximizem a performance dos algoritmos de minera\u00e7\u00e3o."},{"location":"classes/concepts/kdd/kdd/#4-data-mining","title":"4. Data Mining","text":"Descri\u00e7\u00e3o Detalhes Objetivo Aplicar algoritmos para extrair padr\u00f5es, associa\u00e7\u00f5es, clusters, anomalias ou previs\u00f5es dos dados. Atividades - Escolher a t\u00e9cnica de minera\u00e7\u00e3o apropriada com base no objetivo (classifica\u00e7\u00e3o, regress\u00e3o, clustering, regras de associa\u00e7\u00e3o, etc.).- Executar algoritmos, ajustando hiperpar\u00e2metros e validando resultados.- Exemplos de t\u00e9cnicas: Classifica\u00e7\u00e3o: \u00c1rvores de decis\u00e3o, SVM, redes neurais. Clustering: K-means, DBSCAN. Regras de associa\u00e7\u00e3o: Algoritmo Apriori.  Detec\u00e7\u00e3o de anomalias: Isolation Forest. Exemplo Identificar grupos de clientes com comportamento de compra semelhante usando K-means. Ferramentas Weka, RapidMiner, TensorFlow, PyTorch. Desafios Sele\u00e7\u00e3o do algoritmo certo, overfitting, escalabilidade em grandes datasets."},{"location":"classes/concepts/kdd/kdd/#5-pattern-evaluation-and-interpretation","title":"5. Pattern Evaluation and Interpretation","text":"Descri\u00e7\u00e3o Detalhes Objetivo Avaliar a validade, utilidade e novidade dos padr\u00f5es descobertos, interpretando-os no contexto do problema. Atividades - Usar m\u00e9tricas espec\u00edficas para avaliar os padr\u00f5es (e.g., acur\u00e1cia, precis\u00e3o, recall, F1-score para classifica\u00e7\u00e3o; silhouette score para clustering).- Validar os resultados com especialistas do dom\u00ednio ou testes estat\u00edsticos.- Filtrar padr\u00f5es irrelevantes ou redundantes. Exemplo Verificar se um padr\u00e3o como \"clientes que compram X tamb\u00e9m compram Y\" \u00e9 estatisticamente significativo e \u00fatil para estrat\u00e9gias de marketing. Ferramentas Visualiza\u00e7\u00f5es (Matplotlib, Tableau), testes estat\u00edsticos. Desafios Evitar padr\u00f5es esp\u00farios e garantir que os resultados sejam acion\u00e1veis."},{"location":"classes/concepts/kdd/kdd/#6-knowledge-presentation-and-use","title":"6. Knowledge Presentation and Use","text":"Descri\u00e7\u00e3o Detalhes Objetivo Comunicar os resultados de forma clara e utiliz\u00e1-los para tomar decis\u00f5es ou resolver problemas. Atividades - Criar relat\u00f3rios, dashboards ou visualiza\u00e7\u00f5es interativas.- Implementar os padr\u00f5es em sistemas operacionais (e.g., recomenda\u00e7\u00f5es em e-commerce).- Documentar o processo para replicabilidade. Exemplo Um dashboard mostrando clusters de clientes para direcionar campanhas de marketing personalizadas. Ferramentas Power BI, Tableau, relat\u00f3rios em LaTeX ou Markdown. Desafios Traduzir resultados t\u00e9cnicos para stakeholders n\u00e3o t\u00e9cnicos."},{"location":"classes/concepts/kdd/kdd/#caracteristicas-do-processo-kdd","title":"Caracter\u00edsticas do Processo KDD","text":"<ul> <li>Iterativo: As etapas podem ser revisadas v\u00e1rias vezes (e.g., voltar ao pr\u00e9-processamento ap\u00f3s avaliar padr\u00f5es insatisfat\u00f3rios).</li> <li>Interativo: Envolve colabora\u00e7\u00e3o entre analistas, especialistas do dom\u00ednio e sistemas automatizados.</li> <li>Multidisciplinar: Integra conhecimentos de estat\u00edstica, ci\u00eancia da computa\u00e7\u00e3o, aprendizado de m\u00e1quina e dom\u00ednio do problema.</li> <li>Focado no usu\u00e1rio: O sucesso depende de alinhar os padr\u00f5es descobertos com os objetivos do neg\u00f3cio ou pesquisa.</li> </ul>"},{"location":"classes/concepts/kdd/kdd/#consideracoes-adicionais","title":"Considera\u00e7\u00f5es Adicionais","text":"<ul> <li>Desafios Comuns:<ul> <li>Lidar com big data (volume, velocidade, variedade).</li> <li>Garantir privacidade e \u00e9tica no uso de dados (e.g., conformidade com LGPD ou GDPR).</li> <li>Escolher algoritmos que escalem bem e sejam robustos a ru\u00eddos.</li> </ul> </li> <li>Diferen\u00e7a entre KDD e Minera\u00e7\u00e3o de Dados: A minera\u00e7\u00e3o de dados \u00e9 uma etapa espec\u00edfica do KDD, enquanto o KDD abrange o processo completo, desde a sele\u00e7\u00e3o at\u00e9 a aplica\u00e7\u00e3o do conhecimento.</li> <li>Aplica\u00e7\u00f5es: Previs\u00e3o de churn, detec\u00e7\u00e3o de fraudes, recomenda\u00e7\u00e3o de produtos, an\u00e1lise gen\u00f4mica, entre outros.</li> </ul>"},{"location":"classes/concepts/kdd/kdd/#resumo","title":"Resumo","text":"<p>O KDD \u00e9 um processo sistem\u00e1tico para transformar dados brutos em conhecimento acion\u00e1vel. Suas etapas (sele\u00e7\u00e3o, pr\u00e9-processamento, transforma\u00e7\u00e3o, minera\u00e7\u00e3o, avalia\u00e7\u00e3o e apresenta\u00e7\u00e3o) s\u00e3o interdependentes e requerem uma combina\u00e7\u00e3o de t\u00e9cnicas computacionais e conhecimento do dom\u00ednio.</p>"},{"location":"classes/concepts/ml/hierarchical/","title":"Hierarchical","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfrom io import StringIO\n</pre> import matplotlib.pyplot as plt from io import StringIO In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots()\n</pre> fig, ax = plt.subplots() In\u00a0[\u00a0]: Copied! <pre>fig.set_size_inches(8, 8)\n</pre> fig.set_size_inches(8, 8) In\u00a0[\u00a0]: Copied! <pre>circle1 = plt.Circle((0.7, 0.7), 0.7, alpha=0.9, color='orange')\ncircle2 = plt.Circle((0.8, 0.6), 0.5, alpha=0.6, color='gray')\ncircle3 = plt.Circle((.85, .55), 0.35, alpha=0.6, color='green')\ncircle4 = plt.Circle((.9, .5), 0.2, alpha=0.6, color='blue')\n</pre> circle1 = plt.Circle((0.7, 0.7), 0.7, alpha=0.9, color='orange') circle2 = plt.Circle((0.8, 0.6), 0.5, alpha=0.6, color='gray') circle3 = plt.Circle((.85, .55), 0.35, alpha=0.6, color='green') circle4 = plt.Circle((.9, .5), 0.2, alpha=0.6, color='blue') In\u00a0[\u00a0]: Copied! <pre>ax.add_patch(circle1)\nax.add_patch(circle2)\nax.add_patch(circle3)\nax.add_patch(circle4)\n</pre> ax.add_patch(circle1) ax.add_patch(circle2) ax.add_patch(circle3) ax.add_patch(circle4) In\u00a0[\u00a0]: Copied! <pre>ax.text(.7, 1.2, 'Artificial Intelligence', horizontalalignment='center', fontsize=12, color='white')\nax.text(.8, .95, 'Machine Learning', horizontalalignment='center', fontsize=12, color='white')\nax.text(.85, .75, 'Neural Networks', horizontalalignment='center', fontsize=12, color='white')\nax.text(.9, .5, 'Deep Learning', horizontalalignment='center', fontsize=12, color='white')\n</pre> ax.text(.7, 1.2, 'Artificial Intelligence', horizontalalignment='center', fontsize=12, color='white') ax.text(.8, .95, 'Machine Learning', horizontalalignment='center', fontsize=12, color='white') ax.text(.85, .75, 'Neural Networks', horizontalalignment='center', fontsize=12, color='white') ax.text(.9, .5, 'Deep Learning', horizontalalignment='center', fontsize=12, color='white') In\u00a0[\u00a0]: Copied! <pre>ax.set_aspect(1.0)\n</pre> ax.set_aspect(1.0) In\u00a0[\u00a0]: Copied! <pre>plt.xlim(0, 1.4)\nplt.ylim(0, 1.4)\nplt.axis('off')\n</pre> plt.xlim(0, 1.4) plt.ylim(0, 1.4) plt.axis('off') In\u00a0[\u00a0]: Copied! <pre>plt.title('Hierarchical Representation of AI Approaches')\n</pre> plt.title('Hierarchical Representation of AI Approaches') In\u00a0[\u00a0]: Copied! <pre># Adjust layout to prevent overlap\nplt.tight_layout()\n</pre> # Adjust layout to prevent overlap plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre># Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</pre> # Display the plot buffer = StringIO() plt.savefig(buffer, format=\"svg\", transparent=True) print(buffer.getvalue())"},{"location":"classes/concepts/ml/ml/","title":"1.1. Machine Learning","text":"<p>Artificial Intelligence (AI) is a broad field that encompasses various approaches and techniques for creating intelligent systems capable of performing tasks that typically require human intelligence. These tasks include reasoning, learning, perception, and decision-making.</p> <p>AI can be categorized into three main paradigms, each with its own strengths and weaknesses: Symbolic AI, Connectionist AI, and Neuro-Symbolic AI. Each of these paradigms has its own strengths and weaknesses, and they are often used in different contexts depending on the problem being addressed.</p>"},{"location":"classes/concepts/ml/ml/#ai-paradigms","title":"AI Paradigms","text":"Paradigm Description Symbolic AI Focuses on high-level reasoning and knowledge representation using symbols and rules. It excels in tasks requiring logical reasoning, such as theorem proving and expert systems. However, it struggles with perception and learning from raw data. Examples include logic-based systems, expert systems, and knowledge graphs. Connectionist AI Based on artificial neural networks (ANNs), it excels in pattern recognition, learning from large datasets, and handling noisy data. It is particularly effective in tasks like image and speech recognition. However, it often lacks interpretability and struggles with reasoning tasks. Examples include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers. Neuro-Symbolic AI Combines the strengths of both symbolic and connectionist AI, aiming to create systems that can reason about complex problems while also learning from data. It leverages symbolic reasoning capabilities alongside neural networks to enhance interpretability and reasoning abilities. Examples include neuro-symbolic systems that integrate symbolic logic with neural networks, such as knowledge-augmented language models and graph neural networks. 2025-09-03T22:44:52.543378 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p>Neuro-Symbolic AI combines symbolic reasoning with neural networks, leveraging the strengths of both approaches. It aims to create systems that can reason about complex problems while also learning from data.</p> <p>This approach is particularly useful in tasks that require both high-level reasoning and the ability to learn from raw data, such as natural language understanding and complex decision-making.</p> <p>There are several approaches to implementing AI. Machine learning (ML) is one of the most common methods, where algorithms learn from data to make predictions or decisions. Neural networks, a subset of ML, are inspired by the structure and function of the human brain and are particularly effective in tasks like image and speech recognition. Deep learning, a more advanced form of neural networks, uses multiple layers of processing to extract complex patterns from large datasets.</p> 2025-09-03T22:44:52.649067 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p>Artificial Neural Networks (ANNs), or simply neural networks, are computational models inspired by the structure and function of biological neural networks. They consist of interconnected nodes (neurons) that process information in a manner similar to the way neurons in the human brain operate. ANNs are capable of learning from data, making them powerful tools for various tasks such as image recognition, natural language processing, and decision-making.</p> <p>Neural networks are the backbone of many modern AI applications, enabling machines to learn from experience and improve their performance over time. They are particularly effective in handling complex patterns and large datasets, making them suitable for a wide range of applications, from computer vision to speech recognition.</p>"},{"location":"classes/concepts/ml/ml/#milestones","title":"Milestones","text":"<p>Foundations of Neural Networks</p> 1943<p> Laid the theoretical groundwork for ANNs, inspiring future computational models of the brain. Warren McCulloch and Walter Pitts publish a paper introducing the first mathematical model of a neural network, describing neurons as logical decision-making units. McCulloch, &amp; W. S., Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. </p> <p>Turing Test Proposed</p> 1950<p> Established a benchmark for assessing AI capabilities, influencing the philosophical and practical development of AI. Alan Turing publishes \"Computing Machinery and Intelligence,\" proposing the Turing Test to evaluate a machine's ability to exhibit intelligent behavior indistinguishable from that of a human. Turing, A. M. (1950). Computing Machinery and Intelligence. </p> <p>Birth of AI as a Discipline</p> 1956<p> Marked the formal establishment of AI as a field of study, fostering research into machine intelligence. The Dartmouth Conference, organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, coins the term artificial intelligence. McCarthy, J., Minsky, M., Rochester, N., Shannon, C. (1955). Dartmouth Conference Proposal. </p> <p>Perceptron Introduced</p> 1958<p> Pioneered the concept of a simple neural network, laying the foundation for future developments in machine learning and neural networks. Frank Rosenblatt develops the Perceptron, an early artificial neural network capable of learning to classify patterns. It consists of a single layer of output nodes connected to input features, using a step function to produce binary outputs. Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. </p> <p>Limitations of Perceptrons \u2192 AI Winter</p> 1969<p> Highlighted the limitations of early neural networks, leading to a temporary decline in interest in neural networks and AI. Marvin Minsky and Seymour Papert publish \"Perceptrons,\" critiquing the limitations of single-layer perceptrons, particularly their inability to solve non-linearly separable problems like the XOR problem. This work leads to a decline in neural network research for over a decade. Led to the first \"AI winter,\" a period of reduced funding and interest in neural networks, shifting focus to symbolic AI. Minsky, M., Papert, S. (1969). Perceptrons: An Introduction to Computational Geometry. </p> <p>Backpropagation Rediscovered</p> 1986<p> Revived interest in ANNs by overcoming limitations of single-layer perceptrons, paving the way for deep learning. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams publish a paper on backpropagation, enabling training of multi-layer neural networks. Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. </p> <p>Universal Approximation Theorem</p> 1989<p> Established the theoretical foundation for neural networks' ability to approximate any continuous function, leading to the development of deep learning. George Cybenko proves that a feedforward neural network with a single hidden layer can approximate any continuous function on compact subsets of \\(R^n\\) under mild conditions on the activation function. Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. </p> <p>Deep Blue Defeats Chess Champion</p> 1997<p> Showcased the potential of AI in strategic games, leading to advancements in game-playing AI and deep learning. IBM's Deep Blue defeats world chess champion Garry Kasparov in a six-game match, marking a significant milestone in AI's ability to compete with human intelligence in complex tasks. Campbell, M., Hoane, A. J., &amp; Hsu, F. (2002). Deep Blue. </p> <p>Convolutional Neural Networks (CNNs)</p> 1998<p> Revolutionized computer vision and image processing, enabling breakthroughs in object recognition and classification. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton publish a paper on CNNs, introducing the LeNet architecture for handwritten digit recognition. LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. </p> <p>Deep Learning Renaissance</p> 2006<p> Sparked the modern deep learning era by showing that deep networks could be trained efficiently. Geoffrey Hinton and colleagues introduce deep belief networks, demonstrating effective pre-training for deep neural networks. Hinton, G. E., Osindero, S., &amp; Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. </p> <p>AlexNet and the ImageNet Breakthrough</p> 2012<p> Demonstrated the superiority of deep learning in computer vision, leading to widespread adoption. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton\u2019s AlexNet wins the ImageNet competition, achieving unprecedented accuracy in image classification using deep CNNs. Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. </p> <p>Generative Adversarial Networks (GANs)</p> 2014<p> Introduced a novel approach to generative modeling, enabling the creation of realistic synthetic data. Ian Goodfellow and colleagues introduce GANs, a framework for training generative models using adversarial training. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014). Generative adversarial nets. </p> <p>DeepMind\u2019s AlphaGo</p> 2015<p> Showcased deep learning\u2019s ability to tackle complex strategic games, advancing AI research. DeepMind\u2019s AlphaGo, using deep reinforcement learning and neural networks, defeats professional Go player Lee Sedol. Silver, D., et al. (2016). Mastering the game of Go with deep neural networks and tree search. </p> <p>Transformers and Attention Mechanisms</p> 2017<p> Revolutionized natural language processing and sequence modeling, enabling breakthroughs in machine translation and text generation. Ashish Vaswani and colleagues introduce the Transformer architecture, which uses self-attention mechanisms to process sequences in parallel, significantly improving performance in NLP tasks. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., &amp; Polosukhin, I. (2017). Attention is all you need.  </p> <p>BERT and Pre-trained Language Models</p> 2018<p> Set new standards in NLP by introducing pre-training and fine-tuning techniques, enabling models to understand context and semantics better. Jacob Devlin and colleagues introduce BERT (Bidirectional Encoder Representations from Transformers), a pre-trained language model that achieves state-of-the-art results on various NLP benchmarks. Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. </p> : <p>GPT-3 and Large Language Models</p> 2020<p> Showcased the capabilities of large-scale language models, enabling advancements in natural language understanding and generation. OpenAI releases GPT-3, a 175 billion parameter language model, demonstrating impressive performance in various NLP tasks, including text generation, translation, and question answering. Brown, T. B., Mann, B., Ryder, N., Subbiah, S., Kaplan, J., Dhariwal, P., Neelakantan, S., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, D., Litwin, M., Gray, S., Chess, B., Clark, J., Berridge, S., Zaremba, W., &amp; Amodei, D. (2020). Language models are few-shot learners. </p> <p>DALL-E and Image Generation</p> 2021<p> Enabled the generation of high-quality images from textual descriptions, showcasing the potential of multimodal AI. OpenAI introduces DALL-E, a model capable of generating images from textual descriptions, demonstrating the power of combining language and vision. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., &amp; Sutskever, I. (2021). Zero-Shot Text-to-Image Generation. </p>"},{"location":"classes/concepts/ml/ml/#machine-learning","title":"Machine Learning","text":"<p>In the context of AI, machine learning (ML) techniques are used to enable systems to learn from data and improve their performance over time without being explicitly programmed. These techniques allow AI systems to adapt and generalize from examples, making them capable of handling a wide range of tasks, from image recognition to natural language processing.</p> <p>The techniques are often split into two main categories: supervised learning and unsupervised learning.</p> <p>Supervised Learning</p> <p>Supervised learning involves training a model on labeled data, where the input data is paired with the correct output. This allows the model to learn patterns and make predictions based on new, unseen data.</p> <p>This approach is particularly effective when there is a clear relationship between the input features and the output labels, allowing the model to generalize from the training data to make accurate predictions on new data. Examples include classification tasks (e.g., identifying objects in images) and regression tasks (e.g., predicting house prices based on features).</p> <p>Unsupervised Learning</p> <p>Unsupervised learning, on the other hand, involves training a model on unlabeled data, where the model must find patterns and relationships within the data without explicit guidance.</p> <p>This approach is useful for discovering hidden structures in data, such as clusters or groups, without prior knowledge of the labels. It is often used in exploratory data analysis and feature extraction. Examples include clustering tasks (e.g., grouping similar documents) and dimensionality reduction tasks (e.g., reducing the number of features in a dataset while preserving important information).</p> <p>There are also semi-supervised learning techniques, which combine both labeled and unlabeled data to improve model performance. This approach is particularly useful when labeled data is scarce or expensive to obtain, allowing the model to leverage the abundance of unlabeled data to enhance its learning.</p> <p>Also, there are reinforcement learning techniques, where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties. This approach is particularly effective for tasks that involve sequential decision-making, such as game playing or robotic control.</p> <p>Machine learning techniques address a wide range of problems, primarily through classification and regression, which are core supervised learning tasks. Classification involves predicting discrete labels or categories based on input features, while regression focuses on predicting continuous values. These approaches are extensively applied across domains such as image recognition, natural language processing, and time series forecasting. However, machine learning also includes other techniques like clustering, dimensionality reduction, reinforcement learning, and anomaly detection, expanding its applicability to diverse challenges.</p> <p>Few examples of machine learning techniques include:</p> Technique Description Decision Trees A tree-like model used for classification and regression tasks, where each internal node represents a feature, each branch represents a decision rule, and each leaf node represents an outcome. Random Forest An ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting. It works by training multiple decision trees on different subsets of the data and averaging their predictions. Support Vector Machines (SVM) A supervised learning algorithm that finds the optimal hyperplane to separate different classes in the feature space. It is effective for high-dimensional data and can handle both linear and non-linear classification tasks. K-Nearest Neighbors (KNN) A simple algorithm that classifies new instances based on the majority class of their k-nearest neighbors in the feature space. It is a non-parametric method that can be used for both classification and regression tasks. Naive Bayes A probabilistic classifier based on Bayes' theorem, assuming independence between features. It is particularly effective for text classification tasks, such as spam detection and sentiment analysis. Linear Regression A statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. It is commonly used for predicting continuous outcomes based on input features. Logistic Regression A statistical method used for binary classification tasks, where the output is a probability that can be mapped to two classes. It models the relationship between input features and the log-odds of the outcome using a logistic function. K-Means Clustering An unsupervised learning algorithm that partitions data into k clusters based on feature similarity. It iteratively assigns data points to the nearest cluster centroid and updates the centroids until convergence. Principal Component Analysis (PCA) A dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving important features. It identifies the principal components that capture the most variance in the data, making it useful for visualization and feature extraction. Gradient Boosting An ensemble learning technique that builds a series of weak learners (usually decision trees) in a sequential manner, where each new learner corrects the errors of the previous ones. It is effective for both classification and regression tasks and is widely used in machine learning competitions."},{"location":"classes/concepts/ml/ml/#neural-networks","title":"Neural Networks","text":"<p>Neural networks are a class of machine learning models inspired by the structure and function of the human brain. They consist of interconnected nodes (neurons) organized in layers, where each connection has an associated weight that is adjusted during training. Neural networks are particularly effective for tasks involving complex patterns, such as image and speech recognition.</p> <p>Neural networks can be categorized into several types, including: </p> <ul> <li>Feedforward Neural Networks (FNNs): The simplest type of neural network where information flows in one direction, from input to output, without cycles. They are commonly used for tasks like classification and regression.</li> <li>Convolutional Neural Networks (CNNs): Specialized neural networks designed for processing grid-like data, such as images. They use convolutional layers to automatically learn spatial hierarchies of features, making them highly effective for image recognition tasks.</li> <li>Recurrent Neural Networks (RNNs): Neural networks designed for sequential data, such as time series or natural language. They have connections that loop back on themselves, allowing them to maintain a memory of previous inputs. This makes them suitable for tasks like language modeling and speech recognition.</li> <li>Transformers: A type of neural network architecture that uses self-attention mechanisms to process sequences of data. They have revolutionized natural language processing tasks, enabling models like BERT and GPT to achieve state-of-the-art performance in various language understanding tasks.</li> </ul>"},{"location":"classes/concepts/ml/ml/#deep-learning","title":"Deep Learning","text":"<p>Deep learning is a subset of machine learning that focuses on using deep neural networks with many layers to learn complex representations of data. It has achieved remarkable success in various domains, including computer vision, natural language processing, and speech recognition. Deep learning models are capable of automatically learning hierarchical features from raw data, eliminating the need for manual feature engineering. This has led to significant advancements in AI applications, enabling systems to perform tasks that were previously considered challenging or impossible.</p>"},{"location":"classes/concepts/ml/ml/#additional-resources","title":"Additional Resources","text":"<ol> <li> <p>Wiki - Neuro-Symbolic AI \u21a9</p> </li> <li> <p>2020, Forbes - Symbolism Versus Connectionism In AI: Is There A Third Way? \u21a9</p> </li> <li> <p>Garcez, A.d., Lamb, L.C. Neurosymbolic AI: the 3rd wave. Artif Intell Rev 56, 12387\u201312406 (2023). doi.org/10.1007/s10462-023-10448-w \u21a9</p> </li> <li> <p>Hodgkin\u2013Huxley model. Alan Hodgkin and Andrew Huxley develop a mathematical model of the action potential in neurons, describing how neurons transmit signals through electrical impulses. This model is foundational for understanding neural dynamics and influences the development of artificial neural networks. Hodgkin, A. L., Huxley, A. F. (1952). A quantitative description of membrane current and its application to conduction and excitation in nerve. .\u00a0\u21a9</p> </li> <li> <p>Visual Cortex and Monocular Deprivation. David H. Hubel and Torsten N. Wiesel conduct pioneering research on the visual cortex of cats, demonstrating how visual experience shapes neural development. Their work on monocular deprivation shows that depriving one eye of visual input during a critical period leads to permanent changes in the visual cortex, highlighting the importance of experience in neural plasticity. Hubel, D. H., &amp; Wiesel, T. N. (1963). Effects of monocular deprivation in kittens. .\u00a0\u21a9</p> </li> <li> <p>Neocognitron. Kunihiko Fukushima develops the Neocognitron, an early convolutional neural network (CNN) model that mimics the hierarchical structure of the visual cortex. This model is a precursor to modern CNNs and demonstrates the potential of hierarchical feature extraction in image recognition tasks. Fukushima, K. (1980). Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position. .\u00a0\u21a9</p> </li> <li> <p>Hopfield Networks. John Hopfield introduces Hopfield networks, a type of recurrent neural network that can serve as associative memory systems. These networks are capable of storing and recalling patterns, laying the groundwork for later developments in neural network architectures. Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. .\u00a0\u21a9</p> </li> <li> <p>Self-Organizing Maps (SOM). Teuvo Kohonen develops Self-Organizing Maps, a type of unsupervised learning algorithm that maps high-dimensional data onto a lower-dimensional grid. SOMs are used for clustering and visualization of complex data, providing insights into the structure of the data. Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. .\u00a0\u21a9</p> </li> <li> <p>Long Short-Term Memory (LSTM) Networks. Sepp Hochreiter and J\u00fcrgen Schmidhuber introduce LSTM networks, a type of recurrent neural network designed to learn long-term dependencies in sequential data. This architecture addresses the vanishing gradient problem in RNNs, enabling effective modeling of long-term dependencies in sequential data. Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. \".\u00a0\u21a9</p> </li> <li> <p>Residual Networks (ResNets). Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun introduce Residual Networks (ResNets), a deep learning architecture that uses skip connections to allow gradients to flow more easily through deep networks. This architecture enables the training of very deep neural networks, significantly improving performance on image recognition tasks. He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Deep residual learning for image recognition. \u21a9</p> </li> </ol>"},{"location":"classes/concepts/ml/relations/","title":"Relations","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2, venn2_circles\nfrom io import StringIO\n</pre> import matplotlib.pyplot as plt from matplotlib_venn import venn2, venn2_circles from io import StringIO In\u00a0[\u00a0]: Copied! <pre># Create a Venn diagram with two sets (Symbolic AI and Connectionist AI)\nplt.figure(figsize=(9, 9))\n</pre> # Create a Venn diagram with two sets (Symbolic AI and Connectionist AI) plt.figure(figsize=(9, 9)) In\u00a0[\u00a0]: Copied! <pre># Define subsets for Symbolic AI, Connectionist AI, and their overlap\nvenn_diagram = venn2(subsets=(7, 8, 4), \n                     set_labels=('Symbolic AI', 'Connectionist AI'), \n                     alpha=0.5)\n</pre> # Define subsets for Symbolic AI, Connectionist AI, and their overlap venn_diagram = venn2(subsets=(7, 8, 4),                       set_labels=('Symbolic AI', 'Connectionist AI'),                       alpha=0.5) In\u00a0[\u00a0]: Copied! <pre># Customize labels for each region\nvenn_diagram.get_label_by_id('10').set_text('\\n'.join([\n    'Logic-Based Systems',\n    'Propositional Logic',\n    'First-Order Logic',\n    'Expert Systems',\n    'Knowledge Graphs',\n    'Rule-Based Systems',\n    'Ontologies'\n]))  # Symbolic AI only\n</pre> # Customize labels for each region venn_diagram.get_label_by_id('10').set_text('\\n'.join([     'Logic-Based Systems',     'Propositional Logic',     'First-Order Logic',     'Expert Systems',     'Knowledge Graphs',     'Rule-Based Systems',     'Ontologies' ]))  # Symbolic AI only In\u00a0[\u00a0]: Copied! <pre>venn_diagram.get_label_by_id('01').set_text('\\n'.join([\n    'Artificial Neural Networks',\n    'Convolutional Neural Networks',\n    'Recurrent Neural Networks',\n    'Transformers',\n    'Generative Adversarial Networks',\n    'Diffusion Models',\n    'Spiking Neural Networks',\n    'Deep Belief Networks',\n    'Autoencoders'\n]))  # Connectionist AI only\n</pre> venn_diagram.get_label_by_id('01').set_text('\\n'.join([     'Artificial Neural Networks',     'Convolutional Neural Networks',     'Recurrent Neural Networks',     'Transformers',     'Generative Adversarial Networks',     'Diffusion Models',     'Spiking Neural Networks',     'Deep Belief Networks',     'Autoencoders' ]))  # Connectionist AI only In\u00a0[\u00a0]: Copied! <pre>venn_diagram.get_label_by_id('11').set_text('\\n'.join([\n    'Reinforcement Learning\\nw/ Symbolic Planning',\n    'Knowledge-Augmented LLMs',\n    'Graph Neural Networks'\n]))  # Overlap\n</pre> venn_diagram.get_label_by_id('11').set_text('\\n'.join([     'Reinforcement Learning\\nw/ Symbolic Planning',     'Knowledge-Augmented LLMs',     'Graph Neural Networks' ]))  # Overlap In\u00a0[\u00a0]: Copied! <pre># Adjust font size for readability\nfor label in venn_diagram.subset_labels:\n    if label:\n        label.set_fontsize(10)\n</pre> # Adjust font size for readability for label in venn_diagram.subset_labels:     if label:         label.set_fontsize(10) In\u00a0[\u00a0]: Copied! <pre># Add circles outline for clarity\nvenn2_circles(subsets=(7, 8, 4), linestyle='solid', linewidth=1)\n</pre> # Add circles outline for clarity venn2_circles(subsets=(7, 8, 4), linestyle='solid', linewidth=1) In\u00a0[\u00a0]: Copied! <pre># Add title\nplt.title(\"Venn Diagram of Symbolic AI, Connectionist AI, and Overlap\", fontsize=14)\n</pre> # Add title plt.title(\"Venn Diagram of Symbolic AI, Connectionist AI, and Overlap\", fontsize=14) In\u00a0[\u00a0]: Copied! <pre># Add annotation for Other Techniques outside the Venn diagram\nplt.text(0.5, 0, \n         'Other Techniques (Outside Categories):\\n' + \n         '\\n'.join([\n             '- Evolutionary Algorithms',\n             '- Swarm Intelligence',\n             '- Bayesian Networks',\n             '- Markov Models',\n             '- Fuzzy Logic',\n             '- Decision Trees',\n             '- Support Vector Machines'\n         ]),\n         fontsize=10, ha='center', va='top', transform=plt.gca().transAxes)\n</pre> # Add annotation for Other Techniques outside the Venn diagram plt.text(0.5, 0,           'Other Techniques (Outside Categories):\\n' +           '\\n'.join([              '- Evolutionary Algorithms',              '- Swarm Intelligence',              '- Bayesian Networks',              '- Markov Models',              '- Fuzzy Logic',              '- Decision Trees',              '- Support Vector Machines'          ]),          fontsize=10, ha='center', va='top', transform=plt.gca().transAxes) In\u00a0[\u00a0]: Copied! <pre># Add an arrow pointing to the intersection with label \"Neuro-Symbolic AI\"\nplt.annotate('Neuro-Symbolic AI', \n             xy=(0.0, -0.2),  # Approximate center of overlap\n             xytext=(0.0, -0.5),  # Position of text\n             fontsize=12, \n             fontweight='regular',\n             ha='center',\n             va='center',\n            #  bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white'),\n             arrowprops=dict(arrowstyle='-&gt;', connectionstyle='arc3', color='black'))\n</pre> # Add an arrow pointing to the intersection with label \"Neuro-Symbolic AI\" plt.annotate('Neuro-Symbolic AI',               xy=(0.0, -0.2),  # Approximate center of overlap              xytext=(0.0, -0.5),  # Position of text              fontsize=12,               fontweight='regular',              ha='center',              va='center',             #  bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white'),              arrowprops=dict(arrowstyle='-&gt;', connectionstyle='arc3', color='black')) In\u00a0[\u00a0]: Copied! <pre># Adjust layout to prevent overlap\nplt.tight_layout()\n</pre> # Adjust layout to prevent overlap plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre># Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</pre> # Display the plot buffer = StringIO() plt.savefig(buffer, format=\"svg\", transparent=True) print(buffer.getvalue())"},{"location":"classes/dados/conceitos/","title":"Conceitos","text":""},{"location":"classes/dados/conceitos/#conceitos-basicos","title":"Conceitos B\u00e1sicos","text":""},{"location":"classes/dados/conceitos/#banco-de-dados","title":"Banco de Dados","text":"<p>O que \u00e9 um banco de dados?</p> <p>Um banco de dados \u00e9 uma cole\u00e7\u00e3o organizada de dados, que representa algo do mundo real. Exemplos de dados s\u00e3o:</p> <ul> <li> <p>Em uma loja, os dados podem ser clientes, produtos e vendas.</p> </li> <li> <p>Em uma faculdade, podem ser alunos, cursos e notas.</p> </li> </ul> <p>O banco de dados serve para guardar informa\u00e7\u00f5es de forma estruturada, para que elas possam ser consultadas, atualizadas e analisadas facilmente.</p>"},{"location":"classes/dados/conceitos/#abordagens-classicas","title":"Abordagens Cl\u00e1ssicas","text":""},{"location":"classes/dados/conceitos/#abordagem-hierarquica","title":"Abordagem Hier\u00e1rquica","text":""},{"location":"classes/dados/conceitos/#abordagem","title":"Abordagem","text":""},{"location":"classes/decision_tree/decision_tree/","title":"2. Decision Tree","text":"<p>As \u00e1rvores de decis\u00e3o s\u00e3o uma t\u00e9cnica popular de aprendizado de m\u00e1quina supervisionado usada para classifica\u00e7\u00e3o e regress\u00e3o. Elas representam decis\u00f5es e suas poss\u00edveis consequ\u00eancias em uma estrutura hier\u00e1rquica, facilitando a interpreta\u00e7\u00e3o dos resultados.</p> <pre><code>graph TD;\n    T1((Teste 1)) --&gt;|Sim| T2((Teste 2))\n    T1 --&gt;|N\u00e3o| T3((Teste 3))\n    T2 --&gt;|Sim| R1[Resultado 1]\n    T2 --&gt;|N\u00e3o| R2[Resultado 2]\n    T3 --&gt;|Sim| R3[Resultado 3]\n    T3 --&gt;|N\u00e3o| R4[Resultado 4]</code></pre> Exemplo: ir para praia ou n\u00e3o? <p>Ap\u00f3s dias de anota\u00e7\u00f5es sobre o comportamento de uma pessoa, foi poss\u00edvel criar uma tabela com os seguintes registros:</p> Dia Sol? Vento? Praia? 1 Sim Sim N\u00e3o 2 Sim Sim N\u00e3o 3 Sim N\u00e3o Sim 4 N\u00e3o N\u00e3o N\u00e3o 5 N\u00e3o Sim N\u00e3o 6 N\u00e3o N\u00e3o N\u00e3o <p>A partir desses dados, podemos construir uma \u00e1rvore de decis\u00e3o para prever se a pessoa ir\u00e1 \u00e0 praia com base nas condi\u00e7\u00f5es clim\u00e1ticas.</p> <pre><code>graph TD;\n    A((Sol?)) --&gt;|Sim| B((Vento?))\n    A --&gt;|N\u00e3o| C[N\u00e3o ir \u00e0 praia]\n    B --&gt;|Sim| D[N\u00e3o ir \u00e0 praia]\n    B --&gt;|N\u00e3o| E[Ir \u00e0 praia]</code></pre> <p>\u00c1rvore de decis\u00e3o simples para prever se a pessoa ir\u00e1 \u00e0 praia com base nas condi\u00e7\u00f5es clim\u00e1ticas. A partir da pergunta \"Sol?\", a \u00e1rvore se divide em dois caminhos: se h\u00e1 sol, verifica-se se h\u00e1 vento. Se n\u00e3o h\u00e1 sol, a decis\u00e3o \u00e9 n\u00e3o ir \u00e0 praia. Se n\u00e3o h\u00e1 vento, a decis\u00e3o \u00e9 ir \u00e0 praia. Fonte: Didatica Tech - \u00c1rvores de Decis\u00e3o.</p>"},{"location":"classes/decision_tree/decision_tree/#consideracoes","title":"Considera\u00e7\u00f5es","text":"<p>Vantagens</p> <ul> <li>Interpreta\u00e7\u00e3o f\u00e1cil: A estrutura em \u00e1rvore facilita a visualiza\u00e7\u00e3o e compreens\u00e3o das decis\u00f5es tomadas pelo modelo.</li> <li>N\u00e3o requer normaliza\u00e7\u00e3o: \u00c1rvores de decis\u00e3o n\u00e3o s\u00e3o sens\u00edveis \u00e0 escala dos dados, o que significa que n\u00e3o \u00e9 necess\u00e1rio normalizar ou padronizar as vari\u00e1veis.</li> <li>Capacidade de lidar com dados categ\u00f3ricos e num\u00e9ricos: Elas podem trabalhar com ambos os tipos de dados sem necessidade de transforma\u00e7\u00e3o pr\u00e9via.</li> </ul> <p>Desvantagens</p> <ul> <li>Tend\u00eancia ao overfitting: \u00c1rvores de decis\u00e3o podem se ajustar demais aos dados de treinamento, capturando ru\u00eddos e padr\u00f5es irrelevantes.</li> <li>Instabilidade: Pequenas varia\u00e7\u00f5es nos dados podem resultar em \u00e1rvores completamente diferentes, tornando o modelo menos robusto.</li> </ul>"},{"location":"classes/decision_tree/decision_tree/#nomenclatura","title":"Nomenclatura","text":"<p>As \u00e1rvores de decis\u00e3o s\u00e3o compostas por n\u00f3s (representando testes em atributos) e folhas (representando resultados ou classes finais). O processo de constru\u00e7\u00e3o da \u00e1rvore envolve a sele\u00e7\u00e3o do atributo mais informativo para dividir os dados em subconjuntos, minimizando a impureza (e.g., usando medidas como entropia ou \u00edndice Gini)<sup>1</sup>.</p> <p></p> <p>Estrutura de uma \u00c1rvore de Decis\u00e3o: os n\u00f3s representam testes em atributos, enquanto as folhas representam os resultados finais. Fonte: Aulas - \u00c1rvores.</p> <p>O objetivo de uma \u00e1rvore de decis\u00e3o \u00e9 criar uma estrutura que minimize a impureza dos n\u00f3s, resultando em folhas que contenham exemplos da mesma classe ou com valores semelhantes. Isso \u00e9 feito atrav\u00e9s de um processo iterativo de divis\u00e3o dos dados, onde em cada n\u00f3 \u00e9 escolhido o atributo que melhor separa os dados em termos de classe ou valor.</p> <p>Existem algumas m\u00e9tricas comuns usadas para medir a qualidade de uma divis\u00e3o, incluindo:</p> <ul> <li>\u00cdndice Gini: Mede a impureza dos dados, onde um valor de 0 indica pureza total (todos os exemplos pertencem \u00e0 mesma classe).</li> <li>Entropia: Mede a incerteza ou aleatoriedade dos dados, onde uma entropia de 0 indica que todos os exemplos pertencem \u00e0 mesma classe.</li> <li>Ganho de Informa\u00e7\u00e3o: Mede a redu\u00e7\u00e3o da entropia ap\u00f3s a divis\u00e3o dos dados.</li> <li>Redu\u00e7\u00e3o da Vari\u00e2ncia: Usada em \u00e1rvores de decis\u00e3o para regress\u00e3o, mede a redu\u00e7\u00e3o da vari\u00e2ncia dos valores ap\u00f3s a divis\u00e3o dos dados.</li> <li>Chi-quadrado: Usado para medir a independ\u00eancia entre vari\u00e1veis categ\u00f3ricas, ajudando a identificar intera\u00e7\u00f5es significativas entre atributos.</li> </ul> <p>Para o c\u00e1lculo do coeficiente de Gini (mais usado em \u00e1rvores de decis\u00e3o):</p> \\[ g_i = 1 - \\sum_{i=1}^{n} p_i^2 \\] <p>onde \\( p_i \\) \u00e9 a propor\u00e7\u00e3o de cada classe \\( i \\) no conjunto de dados.</p> <p>Mais baixo a impureza, mais puro o n\u00f3.</p>"},{"location":"classes/decision_tree/decision_tree/#construcao","title":"Constru\u00e7\u00e3o","text":"<p>Neste exemplo, temos um conjunto de dados sobre transa\u00e7\u00f5es financeiras, onde cada transa\u00e7\u00e3o \u00e9 classificada como \"Fraude\" ou \"Normal\". A \u00e1rvore de decis\u00e3o pode ser usada para prever se uma nova transa\u00e7\u00e3o \u00e9 fraudulenta ou n\u00e3o, com base em caracter\u00edsticas como o valor da transa\u00e7\u00e3o e o per\u00edodo.</p> <p>Fraude</p> data sample (20/38)plot Valor Periodo Classe 2500 Diurno Normal 1500 Diurno Normal 700 Diurno Normal 3700 Diurno Normal 5600 Diurno Normal 8000 Diurno Normal 3200 Noturno Normal 2900 Noturno Normal 5950 Diurno Normal 630 Noturno Fraude 1800 Diurno Normal 2700 Diurno Normal 3300 Diurno Normal 4200 Noturno Normal 7000 Diurno Normal 500 Diurno Normal 850 Noturno Normal 900 Diurno Normal 4700 Diurno Fraude 5650 Diurno Fraude 2025-09-03T22:44:53.778932 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p>Para construir a \u00e1rvore de decis\u00e3o, os dados s\u00e3o divididos em n\u00f3s com base nas caracter\u00edsticas mais informativas, minimizando a impureza dos n\u00f3s. Para construir a \u00e1rvore, o algoritmo avalia cada atributo e escolhe aquele que melhor separa as classes, utilizando m\u00e9tricas como o \u00edndice Gini ou entropia.</p>"},{"location":"classes/decision_tree/decision_tree/#passo-a-passo","title":"Passo a passo","text":"<ol> <li>Definir o n\u00f3 com os dados daquele ramo.</li> <li>Calcular a impureza de cada atributo.</li> <li>Escolher o atributo que melhor separa os dados.</li> <li>Dividir os dados com base no atributo escolhido.</li> <li>Repetir o processo para cada subconjunto at\u00e9 que um crit\u00e9rio de parada seja atendido (e.g., todos os exemplos em um n\u00f3 pertencem \u00e0 mesma classe ou um n\u00famero m\u00ednimo de exemplos \u00e9 atingido).</li> </ol> <p>Para definir o n\u00f3 raiz, o algoritmo avalia todos os atributos e calcula a impureza de cada um. O atributo com a menor impureza \u00e9 escolhido como o n\u00f3 raiz. Em seguida, os dados s\u00e3o divididos com base nesse atributo, criando ramos na \u00e1rvore. O processo \u00e9 repetido recursivamente para cada ramo at\u00e9 que todos os n\u00f3s sejam folhas (ou seja, n\u00e3o possam ser divididos mais).</p> Feature Sim N\u00e3o Valor &gt;= 3000 18 20 Fraude 5 1 Normal 13 19 Periodo = Noturno 14 24 Fraude 4 2 Normal 10 22 <p>C\u00e1lculo do \u00edndice de Gini para cada crit\u00e9rio sobre atributos:</p> \\[ \\text{Gini}(\\text{Crit\u00e9rio}) = 1 - \\left(\\frac{fraude}{fraude + normal}\\right)^2 - \\left(\\frac{normal}{fraude + normal}\\right)^2  \\] Valor &gt;= 3000Periodo = Noturno \\[ \\text{Gini}(\\text{Valor}\\geq 3000) = 1 - \\left(\\frac{5}{18}\\right)^2 - \\left(\\frac{13}{18}\\right)^2 = 0.4012 \\] \\[ \\text{Gini}(\\text{Valor} &lt; 3000) = 1 - \\left(\\frac{1}{20}\\right)^2 - \\left(\\frac{19}{20}\\right)^2 = 0.0950 \\] <p>Normalizando os valores, temos:</p> \\[ \\text{Pureza do n\u00f3} = \\frac{\\text{18}}{38} \\cdot 0.4012 + \\frac{20}{38} \\cdot 0.0950 = 0.2401 \\] \\[ \\text{Gini}(\\text{Periodo} = \\text{Noturno}) = 1 - \\left(\\frac{4}{14}\\right)^2 - \\left(\\frac{10}{14}\\right)^2 = 0.4082 \\] \\[ \\text{Gini}(\\text{Periodo} \\neq \\text{Noturno}) = 1 - \\left(\\frac{2}{24}\\right)^2 - \\left(\\frac{22}{24}\\right)^2 = 0.1528 \\] <p>Normalizando os valores, temos:</p> \\[ \\text{Pureza do n\u00f3} = \\frac{\\text{14}}{38} \\cdot 0.4082 + \\frac{24}{38} \\cdot 0.1528 = 0.2469 \\] <p>\u00c1rvore de decis\u00e3o resultante pode ser representada da seguinte forma:</p> \u00c1rvore constru\u00eddadecision treecode <pre><code>graph TD;\n    A{Valor &gt;= 3000?} --&gt;|Sim| B{Periodo = Noturno?}\n    A --&gt;|N\u00e3o| C[Normal]\n    B --&gt;|Sim| D[Fraude]\n    B --&gt;|N\u00e3o| E[Normal]</code></pre> <p>Accuracy: 0.62  2025-09-03T22:44:53.946913 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ </p> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\nplt.figure(figsize=(12, 10))\n\ndf = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/fraude.csv')\n\nlabel_encoder = LabelEncoder()\n\n# Carregar o conjunto de dados\nx = df[['Valor', 'Periodo']]\nx['Periodo'] = label_encoder.fit_transform(x['Periodo'])\ny = df['Classe']\n\n# Dividir os dados em conjuntos de treinamento e teste\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n\n# Avaliar o modelo\naccuracy = classifier.score(x_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\ntree.plot_tree(classifier)\n\n# Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n</code></pre>"},{"location":"classes/decision_tree/decision_tree/#implementacao-com-bibliotecas","title":"Implementa\u00e7\u00e3o com Bibliotecas","text":"<p>As \u00e1rvores de decis\u00e3o podem ser implementadas usando bibliotecas populares como <code>scikit-learn</code> em Python, que oferece uma interface simples para criar e treinar modelos de \u00e1rvores de decis\u00e3o. A seguir \u00e9 um exemplo b\u00e1sico de como criar uma \u00e1rvore de decis\u00e3o para classifica\u00e7\u00e3o:</p> <p>Iris Dataset</p> outputdatasetcode <p>Validation Accuracy: 1.0000 Feature Importances: </p> Feature Importance 2 petal length (cm) 0.557274 3 petal width (cm) 0.423616 0 sepal length (cm) 0.019110 1 sepal width (cm) 0.000000 2025-09-03T22:44:54.128956 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ sepal_l sepal_w petal_l petal_w class 6.4 2.9 4.3 1.3 versicolor 4.8 3 1.4 0.1 setosa 6.7 3.1 4.4 1.4 versicolor 5.5 2.4 3.7 1 versicolor 5.7 2.9 4.2 1.3 versicolor 7.2 3 5.8 1.6 virginica 6.7 3.3 5.7 2.5 virginica 4.9 2.4 3.3 1 versicolor 7.7 2.8 6.7 2 virginica 6.3 2.5 4.9 1.5 versicolor 7.4 2.8 6.1 1.9 virginica 6.2 2.9 4.3 1.3 versicolor 4.9 3.6 1.4 0.1 setosa 6 2.2 5 1.5 virginica 5.2 3.4 1.4 0.2 setosa <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score\n\nplt.figure(figsize=(12, 10))\n\n# Carregar o conjunto de dados Iris\niris = load_iris()\nx = iris.data\ny = iris.target\n\n# Dividir os dados em conjuntos de treinamento e teste\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n# Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n\n# Evaluate the model\ny_pred = classifier.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\n\n# Optional: Print feature importances\nfeature_importance = pd.DataFrame({\n    'Feature': iris.feature_names,\n    'Importance': classifier.feature_importances_\n})\nprint(\"&lt;br&gt;Feature Importances:\")\nprint(feature_importance.sort_values(by='Importance', ascending=False).to_html())\n\ntree.plot_tree(classifier)\n\n# Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre> <p>Titanic Dataset</p> decision treedatasetcode <p>Accuracy: 0.78  2025-09-03T22:44:54.741175 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ </p> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 1 0 3 Braund, Mr. Owen Harris male 22 1 0 A/5 21171 7.25 nan S 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 PC 17599 71.2833 C85 C 3 1 3 Heikkinen, Miss. Laina female 26 0 0 STON/O2. 3101282 7.925 nan S 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 113803 53.1 C123 S 5 0 3 Allen, Mr. William Henry male 35 0 0 373450 8.05 nan S <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\n# Preprocess the data\ndef preprocess(df):\n    # Fill missing values\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n\n    # Convert categorical variables\n    label_encoder = LabelEncoder()\n    df['Sex'] = label_encoder.fit_transform(df['Sex'])\n    df['Embarked'] = label_encoder.fit_transform(df['Embarked'])\n\n    # Select features\n    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n    return df[features]\n\nplt.figure(figsize=(12, 10))\n\ndf = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/kaggle/titanic-dataset.csv')\n\n# Carregar o conjunto de dados\nx = preprocess(df)\ny = df['Survived']\n\n# Dividir os dados em conjuntos de treinamento e teste\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n\n# Avaliar o modelo\naccuracy = classifier.score(x_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\ntree.plot_tree(classifier)\n\n# Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n</code></pre>"},{"location":"classes/decision_tree/decision_tree/#exercicio","title":"Exerc\u00edcio","text":"<p>Entrega</p> <p> 29.ago 9:00</p> <p> Individual</p> <p> Entrega do link via Canvas.</p> <p>Dentre os datasets dispon\u00edveis, escolha um cujo objetivo seja prever uma vari\u00e1vel categ\u00f3rica (classifica\u00e7\u00e3o). Utilize o algoritmo de \u00e1rvore de decis\u00e3o para treinar um modelo e avaliar seu desempenho.</p> <p>Utilize as bibliotecas <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> e <code>scikit-learn</code> para auxiliar no desenvolvimento do projeto.</p> <p>A entrega deve ser feita atrav\u00e9s do Canvas - Exerc\u00edcio \u00c1rvore de Decis\u00e3o. S\u00f3 ser\u00e3o aceitos links para reposit\u00f3rios p\u00fablicos do GitHub contendo a documenta\u00e7\u00e3o (relat\u00f3rio) e o c\u00f3digo do projeto. Conforme exemplo do template-projeto-integrador. ESTE EXERC\u00cdCIO \u00c9 INDIVIDUAL.</p> <p>A entrega deve incluir as seguintes etapas:</p> Etapa Crit\u00e9rio Descri\u00e7\u00e3o Pontos 1 Explora\u00e7\u00e3o dos Dados An\u00e1lise inicial do conjunto de dados - com explica\u00e7\u00e3o sobre a natureza dos dados -, incluindo visualiza\u00e7\u00f5es e estat\u00edsticas descritivas. 20 2 Pr\u00e9-processamento Limpeza dos dados, tratamento de valores ausentes e normaliza\u00e7\u00e3o. 10 3 Divis\u00e3o dos Dados Separa\u00e7\u00e3o do conjunto de dados em treino e teste. 20 4 Treinamento do Modelo Implementa\u00e7\u00e3o do modelo Decision Tree. 10 5 Avalia\u00e7\u00e3o do Modelo Avalia\u00e7\u00e3o do desempenho do modelo utilizando m\u00e9tricas apropriadas. 20 6 Relat\u00f3rio Final Documenta\u00e7\u00e3o do processo, resultados obtidos e poss\u00edveis melhorias. Obrigat\u00f3rio: uso do template-projeto-integrador, individual. 20"},{"location":"classes/decision_tree/decision_tree/#adicional","title":"Adicional","text":"<ol> <li> <p>Aulas - \u00c1rvores \u21a9</p> </li> <li> <p>Didatica Tech - \u00c1rvores de Decis\u00e3o \u21a9</p> </li> </ol>"},{"location":"classes/decision_tree/fraude_dataset/","title":"Fraude dataset","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/fraude.csv')\n</pre> df = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/fraude.csv') In\u00a0[\u00a0]: Copied! <pre>print(df.head(20).to_markdown(index=False))\n</pre> print(df.head(20).to_markdown(index=False))"},{"location":"classes/decision_tree/fraude_decision_tree/","title":"Fraude decision tree","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import matplotlib.pyplot as plt import pandas as pd In\u00a0[\u00a0]: Copied! <pre>from io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n</pre> from io import StringIO from sklearn import tree from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(12, 10))\n</pre> plt.figure(figsize=(12, 10)) In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/fraude.csv')\n</pre> df = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/fraude.csv') In\u00a0[\u00a0]: Copied! <pre>label_encoder = LabelEncoder()\n</pre> label_encoder = LabelEncoder() In\u00a0[\u00a0]: Copied! <pre># Carregar o conjunto de dados\nx = df[['Valor', 'Periodo']]\nx['Periodo'] = label_encoder.fit_transform(x['Periodo'])\ny = df['Classe']\n</pre> # Carregar o conjunto de dados x = df[['Valor', 'Periodo']] x['Periodo'] = label_encoder.fit_transform(x['Periodo']) y = df['Classe'] In\u00a0[\u00a0]: Copied! <pre># Dividir os dados em conjuntos de treinamento e teste\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n</pre> # Dividir os dados em conjuntos de treinamento e teste x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) In\u00a0[\u00a0]: Copied! <pre># Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n</pre> # Criar e treinar o modelo de \u00e1rvore de decis\u00e3o classifier = tree.DecisionTreeClassifier() classifier.fit(x_train, y_train) In\u00a0[\u00a0]: Copied! <pre># Avaliar o modelo\naccuracy = classifier.score(x_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\ntree.plot_tree(classifier)\n</pre> # Avaliar o modelo accuracy = classifier.score(x_test, y_test) print(f\"Accuracy: {accuracy:.2f}\") tree.plot_tree(classifier) In\u00a0[\u00a0]: Copied! <pre># Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n</pre> # Para imprimir na p\u00e1gina HTML buffer = StringIO() plt.savefig(buffer, format=\"svg\") print(buffer.getvalue())"},{"location":"classes/decision_tree/fraude_plot/","title":"Fraude plot","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import matplotlib.pyplot as plt import pandas as pd In\u00a0[\u00a0]: Copied! <pre>from io import StringIO\n</pre> from io import StringIO In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(5, 4))\n</pre> plt.figure(figsize=(5, 4)) In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/fraude.csv')\nfraudes = df[df['Classe'] == 'Fraude']\nnormais = df[df['Classe'] == 'Normal']\nplt.plot(\n    normais['Periodo'], normais['Valor'], 'ob',\n    fraudes['Periodo'], fraudes['Valor'], 'or',\n)\n</pre> df = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/fraude.csv') fraudes = df[df['Classe'] == 'Fraude'] normais = df[df['Classe'] == 'Normal'] plt.plot(     normais['Periodo'], normais['Valor'], 'ob',     fraudes['Periodo'], fraudes['Valor'], 'or', ) In\u00a0[\u00a0]: Copied! <pre># Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</pre> # Para imprimir na p\u00e1gina HTML buffer = StringIO() plt.savefig(buffer, format=\"svg\", transparent=True) print(buffer.getvalue())"},{"location":"classes/decision_tree/fraude_tree/","title":"Fraude tree","text":"<pre><code>graph TD;\n    A{Valor &gt;= 3000?} --&gt;|Sim| B{Periodo = Noturno?}\n    A --&gt;|N\u00e3o| C[Normal]\n    B --&gt;|Sim| D[Fraude]\n    B --&gt;|N\u00e3o| E[Normal]</code></pre>"},{"location":"classes/decision_tree/iris_dataset/","title":"Iris dataset","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom sklearn.datasets import load_iris\n</pre> import pandas as pd from sklearn.datasets import load_iris In\u00a0[\u00a0]: Copied! <pre># Carregar o conjunto de dados Iris\niris = load_iris()\n</pre> # Carregar o conjunto de dados Iris iris = load_iris() In\u00a0[\u00a0]: Copied! <pre># Transforma em DataFrame\ndf = pd.DataFrame(\n    data=iris.data,\n    columns=['sepal_l', 'sepal_w', 'petal_l', 'petal_w']\n)\ndf['class'] = iris.target_names[iris.target]\n</pre> # Transforma em DataFrame df = pd.DataFrame(     data=iris.data,     columns=['sepal_l', 'sepal_w', 'petal_l', 'petal_w'] ) df['class'] = iris.target_names[iris.target] In\u00a0[\u00a0]: Copied! <pre># Imprime os dados\nprint(df.sample(frac=.1).to_markdown(index=False))\n</pre> # Imprime os dados print(df.sample(frac=.1).to_markdown(index=False))"},{"location":"classes/decision_tree/iris_decision_tree/","title":"Iris decision tree","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import matplotlib.pyplot as plt import pandas as pd In\u00a0[\u00a0]: Copied! <pre>from io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score\n</pre> from io import StringIO from sklearn import tree from sklearn.model_selection import train_test_split from sklearn.datasets import load_iris from sklearn.metrics import accuracy_score In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(12, 10))\n</pre> plt.figure(figsize=(12, 10)) In\u00a0[\u00a0]: Copied! <pre># Carregar o conjunto de dados Iris\niris = load_iris()\nx = iris.data\ny = iris.target\n</pre> # Carregar o conjunto de dados Iris iris = load_iris() x = iris.data y = iris.target In\u00a0[\u00a0]: Copied! <pre># Dividir os dados em conjuntos de treinamento e teste\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n</pre> # Dividir os dados em conjuntos de treinamento e teste x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42) In\u00a0[\u00a0]: Copied! <pre># Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n</pre> # Criar e treinar o modelo de \u00e1rvore de decis\u00e3o classifier = tree.DecisionTreeClassifier() classifier.fit(x_train, y_train) In\u00a0[\u00a0]: Copied! <pre># Evaluate the model\ny_pred = classifier.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\n</pre> # Evaluate the model y_pred = classifier.predict(x_test) accuracy = accuracy_score(y_test, y_pred) print(f\"Validation Accuracy: {accuracy:.4f}\") In\u00a0[\u00a0]: Copied! <pre># Optional: Print feature importances\nfeature_importance = pd.DataFrame({\n    'Feature': iris.feature_names,\n    'Importance': classifier.feature_importances_\n})\nprint(\"&lt;br&gt;Feature Importances:\")\nprint(feature_importance.sort_values(by='Importance', ascending=False).to_html())\n</pre> # Optional: Print feature importances feature_importance = pd.DataFrame({     'Feature': iris.feature_names,     'Importance': classifier.feature_importances_ }) print(\"Feature Importances:\") print(feature_importance.sort_values(by='Importance', ascending=False).to_html()) In\u00a0[\u00a0]: Copied! <pre>tree.plot_tree(classifier)\n</pre> tree.plot_tree(classifier) In\u00a0[\u00a0]: Copied! <pre># Display the plot\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</pre> # Display the plot buffer = StringIO() plt.savefig(buffer, format=\"svg\", transparent=True) print(buffer.getvalue())"},{"location":"classes/decision_tree/titanic_dataset/","title":"Titanic dataset","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/kaggle/titanic-dataset.csv')\n</pre> df = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/kaggle/titanic-dataset.csv') In\u00a0[\u00a0]: Copied! <pre>print(df.head(5).to_markdown(index=False))\n</pre> print(df.head(5).to_markdown(index=False))"},{"location":"classes/decision_tree/titanic_decision_tree/","title":"Titanic decision tree","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\n</pre> import matplotlib.pyplot as plt import pandas as pd In\u00a0[\u00a0]: Copied! <pre>from io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n</pre> from io import StringIO from sklearn import tree from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score In\u00a0[\u00a0]: Copied! <pre># Preprocess the data\ndef preprocess(df):\n    # Fill missing values\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n    \n    # Convert categorical variables\n    label_encoder = LabelEncoder()\n    df['Sex'] = label_encoder.fit_transform(df['Sex'])\n    df['Embarked'] = label_encoder.fit_transform(df['Embarked'])\n    \n    # Select features\n    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n    return df[features]\n</pre> # Preprocess the data def preprocess(df):     # Fill missing values     df['Age'].fillna(df['Age'].median(), inplace=True)     df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)     df['Fare'].fillna(df['Fare'].median(), inplace=True)          # Convert categorical variables     label_encoder = LabelEncoder()     df['Sex'] = label_encoder.fit_transform(df['Sex'])     df['Embarked'] = label_encoder.fit_transform(df['Embarked'])          # Select features     features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']     return df[features] In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(12, 10))\n</pre> plt.figure(figsize=(12, 10)) In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/kaggle/titanic-dataset.csv')\n</pre> df = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/kaggle/titanic-dataset.csv') In\u00a0[\u00a0]: Copied! <pre># Carregar o conjunto de dados\nx = preprocess(df)\ny = df['Survived']\n</pre> # Carregar o conjunto de dados x = preprocess(df) y = df['Survived'] In\u00a0[\u00a0]: Copied! <pre># Dividir os dados em conjuntos de treinamento e teste\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n</pre> # Dividir os dados em conjuntos de treinamento e teste x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) In\u00a0[\u00a0]: Copied! <pre># Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n</pre> # Criar e treinar o modelo de \u00e1rvore de decis\u00e3o classifier = tree.DecisionTreeClassifier() classifier.fit(x_train, y_train) In\u00a0[\u00a0]: Copied! <pre># Avaliar o modelo\naccuracy = classifier.score(x_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\ntree.plot_tree(classifier)\n</pre> # Avaliar o modelo accuracy = classifier.score(x_test, y_test) print(f\"Accuracy: {accuracy:.2f}\") tree.plot_tree(classifier) In\u00a0[\u00a0]: Copied! <pre># Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n</pre> # Para imprimir na p\u00e1gina HTML buffer = StringIO() plt.savefig(buffer, format=\"svg\") print(buffer.getvalue())"},{"location":"classes/kmeans/kmeans/","title":"5. K-Means","text":"<p>Exerc\u00edcio</p> <p>Utilizando</p>"},{"location":"classes/knn/knn/","title":"4. KNN","text":"<p>K-Nearest Neighbors (KNN)</p> <p>K-Nearest Neighbors (KNN) \u00e9 um algoritmo de aprendizado de m\u00e1quina usado para classifica\u00e7\u00e3o e regress\u00e3o. Ele funciona identificando os K vizinhos mais pr\u00f3ximos de um ponto de dados e fazendo previs\u00f5es com base nesses vizinhos.</p> <p>O KNN \u00e9 um algoritmo baseado em inst\u00e2ncias, o que significa que ele n\u00e3o faz suposi\u00e7\u00f5es sobre a distribui\u00e7\u00e3o dos dados. Em vez disso, ele armazena todos os pontos de dados de treinamento e, durante a previs\u00e3o, calcula a dist\u00e2ncia entre o ponto de dados de entrada e todos os pontos de dados de treinamento. Os K pontos mais pr\u00f3ximos s\u00e3o ent\u00e3o usados para fazer a previs\u00e3o, geralmente por meio de vota\u00e7\u00e3o (no caso de classifica\u00e7\u00e3o) ou m\u00e9dia (no caso de regress\u00e3o).</p> <p>Uma das principais vantagens do KNN \u00e9 sua simplicidade e facilidade de implementa\u00e7\u00e3o. No entanto, ele pode ser computacionalmente caro, especialmente com grandes conjuntos de dados, e sua performance pode ser afetada pela escolha da m\u00e9trica de dist\u00e2ncia e pelo valor de K.</p>"},{"location":"classes/knn/knn/#implementacao-do-knn","title":"Implementa\u00e7\u00e3o do KNN","text":"<p>A implementa\u00e7\u00e3o do KNN pode ser feita de v\u00e1rias maneiras, mas uma das mais comuns \u00e9 usar a biblioteca <code>scikit-learn</code> em Python. Aqui est\u00e1 um exemplo b\u00e1sico de como implementar o KNN para classifica\u00e7\u00e3o:</p> <p>Exerc\u00edcio</p> <p>Utilizando</p>"},{"location":"classes/metrics/metricas/","title":"6. Evaluation Metrics","text":"<p>Exerc\u00edcio</p> <p>Utilizando</p>"},{"location":"classes/page_rank/page_rank/","title":"9. Page Rank","text":"<ul> <li>Introdu\u00e7\u00e3o ao Page Rank: .pagerank/introducao.md</li> <li>Algoritmo Page Rank: .pagerank/algoritmo.md</li> <li>Aplica\u00e7\u00f5es do Page Rank: .pagerank/aplicacoes.md</li> <li>Avalia\u00e7\u00e3o do Page Rank: .pagerank/avaliacao.md</li> <li>Implementa\u00e7\u00e3o do Page Rank: .pagerank/implementacao.md</li> <li>Exemplos de Page Rank: .pagerank/exemplos.md</li> </ul> <p>Exerc\u00edcio</p> <p>Utilizando</p>"},{"location":"classes/preprocessing/ok/","title":"Ok","text":"<ul> <li>Introdu\u00e7\u00e3o ao Aprendizado de M\u00e1quina: .ml/introducao.md</li> <li>Tipos de Aprendizado de M\u00e1quina: .ml/tipos_aprendizado.md</li> <li>Algoritmos de Classifica\u00e7\u00e3o: .ml/classificacao.md</li> <li>Algoritmos de Regress\u00e3o: .ml/regressao.md</li> <li> <p>Algoritmos de Agrupamento: .ml/agrupamento.md</p> </li> <li> <p>Introdu\u00e7\u00e3o: .kdd/introducao.md</p> </li> <li>Processo de KDD: .kdd/processo.md</li> <li>Pr\u00e9-processamento: .kdd/preprocessamento.md</li> <li>An\u00e1lise Explorat\u00f3ria: .kdd/analise_exploratoria.md</li> <li>Visualiza\u00e7\u00e3o de Dados: .kdd/visualizacao_dados.md</li> <li>Feature Engineering: .kdd/feature_engineering.md</li> <li>Feature Selection: .kdd/feature_selection.md</li> <li>Feature Extraction: .kdd/feature_extraction.md</li> <li>Modelagem: .kdd/modelagem.md</li> <li>Avalia\u00e7\u00e3o de Modelos: .kdd/avaliacao_modelos.md</li> <li>Valida\u00e7\u00e3o Cruzada: .kdd/validacao_cruzada.md</li> <li>Overfitting e Underfitting: .kdd/overfitting_underfitting.md</li> <li>Pipeline de Machine Learning: .kdd/pipeline_ml.md</li> <li>Deploy de Modelos: .kdd/deploy_modelos.md</li> <li>Monitoramento de Modelos: .kdd/monitoramento_modelos.md</li> </ul>"},{"location":"classes/preprocessing/preprocessing/","title":"3. Preprocessing","text":"<p>Structured data</p> <p>Unstructured data</p> <p>Controle sobre o experimento</p> <p>Exerc\u00edcio</p> <p>Utilizando dados estruturados e n\u00e3o estruturados, crie um conjunto de dados fict\u00edcio para treinar um modelo de aprendizado de m\u00e1quina.</p>"},{"location":"classes/pyspark/pyspark/","title":"10. PySpark","text":"<ul> <li>Introdu\u00e7\u00e3o ao Page Rank: .pagerank/introducao.md</li> <li>Algoritmo Page Rank: .pagerank/algoritmo.md</li> <li>Aplica\u00e7\u00f5es do Page Rank: .pagerank/aplicacoes.md</li> <li>Avalia\u00e7\u00e3o do Page Rank: .pagerank/avaliacao.md</li> <li>Implementa\u00e7\u00e3o do Page Rank: .pagerank/implementacao.md</li> <li>Exemplos de Page Rank: .pagerank/exemplos.md</li> </ul> <p>Exerc\u00edcio</p> <p>Utilizando</p>"},{"location":"classes/random_forest/random_forest/","title":"7. Random Forest","text":"<ul> <li>Introdu\u00e7\u00e3o ao Random Forest: .random_forest/introducao.md</li> <li>Constru\u00e7\u00e3o de Random Forest: .random_forest/construcao.md</li> <li>Avalia\u00e7\u00e3o de Random Forest: .random_forest/avaliacao.md</li> </ul> <p>Exerc\u00edcio</p> <p>Utilizando</p>"},{"location":"classes/sql/consultas/","title":"Consultas","text":""},{"location":"classes/sql/introducao/","title":"Introdu\u00e7\u00e3o","text":""},{"location":"classes/sql/join/","title":"Join","text":""},{"location":"classes/svm/svm/","title":"8. SVM","text":"<ul> <li>Introdu\u00e7\u00e3o ao SVM: .svm/introducao.md</li> <li>Constru\u00e7\u00e3o de SVM: .svm/construcao.md</li> <li>Avalia\u00e7\u00e3o de SVM: .svm/avaliacao.md</li> <li>SVM com Kernel: .svm/kernel.md</li> </ul> <p>Exerc\u00edcio</p> <p>Utilizando</p>"}]}